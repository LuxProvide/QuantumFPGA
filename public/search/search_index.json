{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Quantum Exact Simulation with Intel\u00ae FPGA","text":"<p>Quantum computing exact simulations involve the use of classical computers to model and analyze the behavior of quantum systems and quantum algorithms, bridging the gap between theoretical quantum mechanics and practical quantum computing technologies. These simulations are crucial for developing, testing, and optimizing quantum algorithms before they are run on actual quantum hardware, which is often limited in availability and capability.</p>"},{"location":"#existing-simulators","title":"Existing simulators","text":"<p>There are several quantum simulators available that vary in their approach, capabilities, and the scale of systems they can simulate. Among the most famous one, you have:</p> <ul> <li> <p>Qiskit Aer: Developed by IBM, Qiskit Aer is an open-source simulator that allows users to perform realistic simulations of quantum circuits, complete with noise models and resource estimations. It helps in understanding how quantum algorithms will perform on real quantum hardware.</p> </li> <li> <p>Cirq: Google's Cirq is another open-source framework designed to simulate and test quantum algorithms on local machines. It is particularly tailored for noisy intermediate-scale quantum (NISQ) computers.</p> </li> <li> <p>cuQuantum: NVIDIA's specialized SDK (Software Development Kit) for accelerating quantum computing simulations on GPUs. Announced and developed by NVIDIA, this toolkit is designed to harness the parallel processing capabilities of GPUs to speed up the simulation of quantum circuits and quantum systems. cuQuantum targets both researchers and developers in the field of quantum computing, providing tools and libraries optimized for NVIDIA's GPU architecture.</p> </li> </ul> <p>Simulations play a dual role in the quantum computing landscape. They are instrumental in:</p> <ul> <li> <p>Algorithm Development: By simulating the outcomes of quantum algorithms, researchers can identify potential improvements and optimizations without needing access to quantum processors.</p> </li> <li> <p>Hardware Design: Simulations help predict how quantum devices might perform in the real world, aiding in the design and construction of more effective quantum hardware.</p> </li> </ul>"},{"location":"#hardware-accelerators-ha","title":"Hardware accelerators (HA)","text":"<p>Hardware accelerators provide significant advantages for simulating quantum systems due to their powerful parallel processing capabilities. In the field of quantum computing, where simulating quantum phenomena on classical computers can be computationally intensive, HA help in addressing some of these challenges by accelerating calculations.</p> <ul> <li> <p>Parallelism: Quantum simulations involve operations on large vectors and matrices since the state of a quantum system is represented by a state vector in a complex vector space, and operations on these states are represented by matrices. For example, GPUs are well-suited for these tasks due to their highly parallel architecture, allowing for faster processing of these large-scale linear algebra operations compared to traditional CPUs.</p> </li> <li> <p>Scalability: While the exponential growth of the quantum state space with each added qubit remains a challenge, HA help push the boundaries of what size systems can be simulated. They enable researchers to simulate slightly larger quantum systems than would be feasible with CPUs alone.</p> </li> <li> <p>Efficiency: For specific types of quantum simulations, such as those involving tensor networks or state vector simulations, HA can significantly speed up the computation. This efficiency is crucial for exploring more complex quantum algorithms and systems within practical time frames.</p> </li> <li> <p>Although less known than GPUs, FPGAs (Field-Programmable Gate Arrays) represent a growing area of interest in the field of quantum computing due to FPGAs' unique properties.</p> </li> <li> <p>FPGAs are integrated circuits that can be configured by the user after manufacturing, allowing for highly specialized hardware setups tailored to specific computational tasks.</p> </li> </ul> <p></p>"},{"location":"#why-using-fpgas-for-quantum-simulations","title":"Why using FPGAs for Quantum Simulations?","text":"<ul> <li> <p>Customizability and Reconfigurability: Unlike CPUs and GPUs, which have fixed architectures, FPGAs can be programmed to create custom hardware configurations. This allows for the optimization of specific algorithms or processes, which can be particularly beneficial for quantum simulations, where different algorithms might benefit from different hardware optimizations.</p> </li> <li> <p>Parallel Processing: FPGAs can be designed to handle parallel computations natively using pipeline parallelism.</p> </li> <li> <p>Low Latency and High Throughput: FPGAs can provide lower latency than CPUs and GPUs because they can be programmed to execute tasks without the overhead of an operating system or other software layers. This makes them ideal for real-time processing and simulations.</p> </li> <li> <p>Energy Efficiency: FPGAs can be more energy-efficient than GPUs and CPUs for certain tasks because they can be stripped down to only the necessary components required for a specific computation, reducing power consumption.</p> </li> </ul>"},{"location":"#intel-fpga-sdk-oneapi-for-fpga","title":"Intel\u00ae FPGA SDK &amp; oneAPI for FPGA","text":"<ul> <li>The Intel\u00ae FPGA Software Development Kit (SDK) provides a comprehensive set of development tools and libraries specifically designed to facilitate the design, creation, testing, and deployment of applications on Intel's FPGA hardware. The SDK includes tools for both high-level and low-level programming, including support for hardware description languages like VHDL and Verilog, as well as higher-level abstractions using OpenCL or HLS (High-Level Synthesis). This makes it easier for developers to leverage the power of FPGAs without needing deep expertise in hardware design.</li> </ul> <ul> <li>Intel\u00ae oneAPI is a unified programming model designed to simplify development across diverse computing architectures\u2014CPUs, GPUs, FPGAs, and other accelerators. The oneAPI for FPGA component specifically targets the optimization and utilization of Intel FPGAs. It allows developers to use a single, consistent programming model to target various hardware platforms, facilitating easier code reuse and system integration. oneAPI includes specialized libraries and tools that enable developers to maximize the performance of their applications on Intel FPGAs while maintaining a high level of productivity and portability.</li> </ul> <p>In this course, you will learn to:</p> <ul> <li> <p>How to use Meluxina's FPGA, i.e., Intel\u00ae FPGA</p> </li> <li> <p>How to exploit FPGA for quantum simulation</p> </li> <li> <p>How to take advantage of the Intel\u00ae oneAPI to code quantum circuit</p> </li> </ul> <p>Remark</p> <p>This course is not intended to be exhaustive. In addition, the described tools and features are constantly evolving. We try our best to keep it up to date. </p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<ul> <li> <p>This course is for students, researchers, enginners wishing to discover how to use oneAPI to program FPGA in this fantastic fields which is Quantum Computing. Participants should still have some experience with Python &amp; modern C++ (e.g., Lambdas, class deduction templates).</p> </li> <li> <p>This course is NOT a Quantum Computing course but intends to show you how to use QC simulation on Meluxina's FPGA.</p> </li> <li> <p>We strongly recommend to interested particpants this [CERN online course)(https://indico.cern.ch/event/970903/).</p> </li> </ul>"},{"location":"#about-this-course","title":"About this course","text":"<p>This course has been developed by the Supercomputing Application Services group at LuxProvide. </p>"},{"location":"compile/","title":"Compiling SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"compile/#setups","title":"Setups","text":"<p>Please clone first the oneAPI-sample repository with the <code>git clone https://github.com/oneapi-src/oneAPI-samples.git</code> in your home folder.</p> <p>Once the repository cloned, you should see the following hierarchy:</p> <pre><code>$ tree -d -L 2 oneAPI-samples\noneAPI-samples\n\u251c\u2500\u2500 AI-and-Analytics\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 End-to-end-Workloads\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Features-and-Functionality\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Getting-Started-Samples\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 images\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Jupyter\n\u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stb\n\u251c\u2500\u2500 DirectProgramming\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++SYCL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++SYCL_FPGA\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Fortran\n\u251c\u2500\u2500 Libraries\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneCCL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDAL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDNN\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDPL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneMKL\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 oneTBB\n\u251c\u2500\u2500 Publications\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 DPC++\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 GPU-Opt-Guide\n\u251c\u2500\u2500 RenderingToolkit\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GettingStarted\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Tutorial\n\u251c\u2500\u2500 Templates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 cmake\n\u2514\u2500\u2500 Tools\n    \u251c\u2500\u2500 Advisor\n    \u251c\u2500\u2500 ApplicationDebugger\n    \u251c\u2500\u2500 Benchmarks\n    \u251c\u2500\u2500 GPU-Occupancy-Calculator\n    \u251c\u2500\u2500 Migration\n    \u2514\u2500\u2500 VTuneProfiler\n</code></pre> <ul> <li>As you can see Intel provides numerous code samples and examples to help your grasping the power of the oneAPI toolkit. </li> <li>We are going to focus on <code>DirectProgramming/C++SYCL_FPGA</code>.</li> <li> <p>Create a symbolic at the root of your home directory pointing to this folder: <pre><code>$ cd\n$ ln -s oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/GettingStarted\n$ tree -d -L 2 GettingStarted\nGettingStarted\n\u251c\u2500\u2500 fast_recompile\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 src\n\u251c\u2500\u2500 fpga_compile\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part1_cpp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part2_dpcpp_functor_usm\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part3_dpcpp_lambda_usm\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part4_dpcpp_lambda_buffers\n\u2514\u2500\u2500 fpga_template\n    \u2514\u2500\u2500 src\n</code></pre></p> </li> <li> <p>The fpga_compile folder provides basic examples to start compiling SYCL C++ code with the DPC++ compiler</p> </li> <li> <p>The fpga_recompile folder show you how to recompile quickly your code without having to rebuild the FPGA image</p> </li> <li> <p>The fpga_template is a starting template project that you can use to bootstrap a project</p> </li> </ul>"},{"location":"compile/#discovering-devices","title":"Discovering devices","text":"<p>Before targeting a specific hardware accelerator, you need to ensure that the sycl runtime is able to detect it.</p> <p>Commands</p> <pre><code># Create permanent tmux session\ntmux new -s fpga_session\n# We need a job allocation on a FPGA node\nsalloc -A p200117 -t 48:00:00 -q default -p fpga -N 1\n# In order to use the  intel-compiler-2023.2.1\nmodule use /project/home/p200117/apps/u100057/easybuild/modules/all\nmodule load 520nmx/20.4\n# The fpga_compile version setup all necessary environment variable to compile code\nmodule load intel-compilers/2023.2.1-fpga_compile\nsycl-ls\n</code></pre> <p>Output</p> <pre><code>[opencl:cpu:0] Intel(R) OpenCL, AMD EPYC 7452 32-Core Processor                 3.0 [2022.13.3.0.16_160000]\n[opencl:acc:1] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2022.13.3.0.16_160000]\n[opencl:acc:2] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0) 1.0 [2022.1]\n[opencl:acc:3] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie1) 1.0 [2022.1]\n</code></pre> <ul> <li>If you see the same output, you are all setup.</li> </ul>"},{"location":"compile/#compilation-manually","title":"Compilation (manually)","text":"<ul> <li>Recalling that full compilation can take hours depending on your application size.</li> <li>In this context, emulation and static report evaluation are keys to succeed in FPGA programming</li> </ul> <p>Full compilation &amp; hardware profiling</p> <p>Don't try a classical debug approach while hoping to solve a problem using multiple design iterations in this condition.  HLS-FPGA programming can be very tedious but SYCL simplifies greatly the process.   </p>"},{"location":"compile/#first-code","title":"First code","text":"<p>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src/vector_add.cpp</p> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        h.single_task&lt;VectorAddID&gt;([=]() {\n          VectorAdd(&amp;accessor_a[0], &amp;accessor_b[0], &amp;accessor_c[0], kVectSize);\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>The <code>vector_add.cpp</code> source file contains all the necessary to understand how to create a SYCL program</p> </li> <li> <p>lines 4 and 5 are the minimal headers to include in your SYCL program</p> </li> <li> <p>line 9 is a forward declaration of the kernel name</p> </li> <li> <p>lines 11-19 is a function representing our kernel. Note the absence of <code>__kernel</code>, <code>__global</code> as it exists in OpenCL</p> </li> <li> <p>lines 30-36 are pragmas defining whether you want a full compilation, a CPU emulation or the simulator</p> </li> <li> <p>line 39 is the queue creation. The queue is bounded to a device. We will discuss it later in details.</p> </li> <li> <p>lines 41-46 provides debugging information at runtime.</p> </li> <li> <p>lines 48-54 instantiates 3 vectors. <code>vec_a</code> and <code>vec_b</code> are input C++ arrays and are initialized inside the next loop. <code>vec_c</code> is an output C++ array collecting computation results between <code>vec_a</code> and <code>vec_b</code>.</p> </li> <li> <p>lines 60-62 create buffers for each vector and specify their size. The runtime copies the data to the FPGA global memory when the kernel starts</p> </li> <li> <p>line 64 submits a command group to the device queue</p> </li> <li> <p>lines 66-68 relies on accessor to infer data dependencies. \"read_only\" accessor have to wait for data to be fetched. \"no_init\" option indicates ito the runtime know that the previous contents of the buffer can be discarded</p> </li> <li> <p>lines 70-73 starts a single tasks (single work-item) and call the kernel function</p> </li> <li> <p>lines 99-105 catch SYCL exceptions and terminate the execution</p> </li> </ul>"},{"location":"compile/#emulation","title":"Emulation","text":"<ul> <li> <p>FPGA emulation refers to the process of using a software or hardware system to mimic the behavior of an FPGA device. This is usually done to test, validate, and debug FPGA designs before deploying them on actual hardware. The Intel\u00ae FPGA emulator runs the code on the host cpu.</p> </li> <li> <p>Emulation is crucial to validate the functionality of your kernel design. </p> </li> <li> <p>During emulation, your are not seeking for performance.</p> </li> </ul> <p>Compile for emulation (in one step)</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes vector_add.cpp -o vector_add.fpga_emu\n</code></pre> <p>Intel uses the SYCL Ahead-of-time (AoT) compilation which as two steps:</p> <ol> <li> <p>The \"compile\" stage compiles the device code to an intermediate representation (SPIR-V).</p> </li> <li> <p>The \"link\" stage invokes the compiler's FPGA backend before linking.</p> </li> </ol> <p>Two-steps compilation</p> <pre><code># Compile \n$ icpx -fsycl -fintelfpga -qactypes -o vector_add.cpp.o -c vector_add.cpp\n# Link\n$ icpx -fsycl -fintelfpga -qactypes vector_add.cpp.o -o vector_add.fpga_emu\n</code></pre> <p>Practical session</p> <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Apply the previous command (single or two steps). What do you read ?    </li> </ul> <ul> <li>The compiler option <code>-qactypes</code> informs the compiler to sreahc and include the Algorithmic C (AC) data type folder for header and libs to the AC data types libraries for Field Programmable Gate Array (FPGA) and CPU compilations.</li> <li>The Algorithmic C (AC) datatypes libraries include a numerical set of datatypes and an interface datatype for modeling channels in communicating processes in C++.</li> </ul>"},{"location":"compile/#static-reports","title":"Static reports","text":"<ul> <li> <p>During the process of compiling an FPGA hardware image with the Intel\u00ae oneAPI DPC++/C++ Compiler, various checkpoints are provided at different compilation steps. These steps include object files generation, an FPGA early image object generation, an FPGA image object generation, and finally executables generation. These checkpoints offer the ability to review errors and make modifications to the source code without needing to do a full compilation every time. </p> </li> <li> <p>When you reach the FPGA early image object checkpoint, you can examine the optimization report generated by the compiler. </p> </li> <li> <p>Upon arriving at the FPGA image object checkpoint, the compiler produces a finished FPGA image.</p> </li> </ul> <p>In order to generate the FPGA early image, you will need to add the following option:</p> <ul> <li> <p><code>-Xshardware</code></p> </li> <li> <p><code>-Xstarget=&lt;target&gt;</code> or <code>-Xsboard=&lt;board&gt;</code></p> </li> <li> <p><code>-fsycl-link=early</code></p> </li> </ul> <p>Compile for FPGA early image</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 vector_add.cpp -o vector_add_report.a\n</code></pre> <ul> <li>The <code>vector_add_report.a</code> is not what we target in priority. We target the reports directory <code>vector_add_report.prj</code> which has been created.</li> </ul> <p>Practical session</p> <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Apply the previous command to the single compilation command</li> </ul> <ul> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code>"},{"location":"compile/#full-compilation","title":"Full compilation","text":"<p>This phase produces the actual FPGA bitstream, i.e., a file containing the programming data associated with your FPGA chip. This file requires the target FPGA platform to be generated and executed. For FPGA programming, the Intel\u00ae oneAPI toolkit requires the Intel\u00ae Quartus\u00ae Prime software to generate this bitstream.</p> <p>Full hardware compilation</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -Xstarget=Stratix10 -DFPGA_HARDWARE vector_add.cpp -o vector_add_report.fpga\n</code></pre> <ul> <li> <p>The compilation will take several hours. Therefore, we strongly advise you to verify your code through emulation first.</p> </li> <li> <p>You can also use the <code>-Xsfast-compile</code> option which offers a faster compile time but reduce the performance of the final FPGA image.</p> </li> </ul>"},{"location":"compile/#fast-recompilation","title":"Fast recompilation","text":"<ul> <li> <p>At first glance having a single source file is not necessarily a good idea when host and device compilation differs so much</p> </li> <li> <p>However, there is two different strategies to deal with it:</p> </li> <li> <p>Use a single source file and add the <code>-reuse-exe</code></p> </li> <li> <p>Separate host and device code compilation in your FPGA project</p> </li> <li> <p>This is up to you to choose the method that suits you the most</p> </li> </ul> <p>Using the <code>-reuse-exe</code> option</p> <p><pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -Xstarget=Stratix10 -DFPGA_HARDWARE -reuse-exe=vector_add.fpga vector_add.cpp -o vector_add.fpga\n</code></pre> If only the host code changed since the previous compilation, providing the <code>-reuse-exe=image</code> flag to <code>icpx</code> instructs the compiler to extract the compiled FPGA binary from the existing executable and package it into the new executable, saving the device compilation time.</p> <p>Question</p> <ul> <li>What happens if the vector_add.fpga is missing ?</li> </ul> <p>Separating host and device code</p> <p>Go to the <code>GettingStarted/fpga_recompile</code> folder. It provides an example of separate host and device code The process is similar as the compilation process for OpenCL except that a single tool is used, i.e., <code>icpx</code></p> <ol> <li>Compile the host code: <pre><code>$ icpx -fsycl -fintelfpga -DFPGA_HARDWARE host.cpp -c -o host.o\n</code></pre></li> <li>Compile the FPGA image: <pre><code>$ icpx -fsycl -fintelfpga -Xshardware -Xstarget=Stratix10 -fsycl-link=image kernel.cpp -o dev_image.a\n</code></pre></li> <li>Link both: <pre><code>$ icpx -fsycl -fintelfpga host.o dev_image.a -o fast_recompile.fpga\n</code></pre></li> </ol>"},{"location":"compile/#summary","title":"Summary","text":"<p>We have seen</p> <ul> <li>How to discover devices on your system</li> <li>How to manually compile a SYCL program<ul> <li>for emulation</li> <li>for early reporting  </li> <li>full hardware compilation</li> </ul> </li> <li>How to perform fast recompilation</li> </ul> <p>We did not see</p> <ul> <li>Compilation for hardware simulation</li> <li>Advanced compilation with multiple source files</li> <li>Extract the bitstream from the final executable</li> </ul>"},{"location":"dpcpp/","title":"What is the Intel\u00ae oneAPI DPC++ compiler","text":"<p>In heterogenous computing, accelerator devices support the host processor by executing specific portion of code more efficiently. In this context, the Intel\u00ae oneAPI toolkit supports two different approaches for heterogeous computing:</p> <p>1. Data Parallel C++ with SYCL</p> <p>SYCL (Specification for Unified Cross-platform C++) provides a higher-level model for writing standard ISO C++ code that is both performance-oriented and portable across various hardware, including CPUs, GPUs and FPGAs It enables the use of standard C++ with extensions to leverage parallel hardware. Host and kernel code share the same source file. The DPC++ compiler is adding SYCL support on top of the LLVM C++ compiler. DPC++ is distributed with the Intel\u00ae oneAPI toolkit.</p> <p>2. OpenMP for C, C++, and Fortran </p> <p>For more than two decades, OpenMP has stood as a standard programming language, with Intel implementing its 5<sup>th</sup> version. The Intel oneAPI C++ Compiler, which includes support for OpenMP offloading, can be found in the Intel oneAPI Base Toolkit, Intel oneAPI HPC Toolkit, and Intel oneAPI IoT Toolkit. Both the Intel\u00ae Fortran Compiler Classic and the Intel\u00ae Fortran Compiler equipped with OpenMP offload support are accessible through the Intel oneAPI HPC Toolkit. </p> <p>Note: OpenMP is not supported for FPGA devices.</p>"},{"location":"dpcpp/#dpc-is-one-of-the-existing-sycl-implementations","title":"DPC++ is one of the existing SYCL implementations","text":"<p>ComputeCpp (codeplay)</p> <p>support will no longer be provided from September 1<sup>st</sup> 2023 (see announce)</p>"},{"location":"dpcpp/#key-features-and-components","title":"Key Features and Components","text":"<ul> <li>Heterogeneous Support: Enables coding for various types of processors within the same program.</li> <li>Performance Optimization: It offers various optimization techniques to ensure code runs as efficiently as possible.</li> <li>Standard Compliance: Aligns with the latest C++ standards, along with the SYCL standard.</li> <li>Debugging and Analysis Tools: Integrates with tools that assist in debugging and analyzing code.</li> <li>Integration with IDEs: Compatible with popular Integrated Development Environments to facilitate a seamless coding experience.</li> <li>Open Source and Community Driven: This promotes collaboration and ensures that the technology stays up to date with industry needs.</li> </ul>"},{"location":"dpcpp/#sycl-and-fpga","title":"SYCL and FPGA","text":"<p>SYCL offers APIs and abstractions, but FPGA cards are unique to each vendor, and even within the same vendor, FPGA cards may have diverse capabilities. DPC++ targets Intel\u00ae FPGA cards specifically and extends SYCL's functions. This allows it to leverage the strength of FPGA, all the while maintaining as much generalizability and portability as possible.</p>"},{"location":"dpcpp/#references","title":"References","text":"<ul> <li>Data Parallel C++: Mastering DPC++ for Programming of Heterogeneous Systems using C++ and SYCL</li> <li>Intel\u00ae oneAPI DPC++/C++ Compiler </li> <li>SYCL official documentation</li> </ul>"},{"location":"intro/","title":"FPGA computing for the HPC ecosystem","text":""},{"location":"meluxina/","title":"Using the Luxembourgish National Supercomputer Meluxina","text":"<p>If you follow this course through the workshop organized by Supercomputing Luxembourg, an allocation on Meluxina has been provided to you by LuxProvide.</p> <p>During this workshop, we will stringly rely on the National Supercomputer. Meluxina is a supercomputer located in Luxembourg, which began operation in 2021. It is part of the European High-Performance Computing (EuroHPC) Joint Undertaking, an initiative by the European Union to develop a world-class supercomputing ecosystem in Europe.</p> <p>The Meluxina supercomputer is hosted by LuxProvide, the national HPC organization in Luxembourg. Its computing power is intended to be used for a wide range of tasks, such as data visualization, artificial intelligence, and simulating complex systems, serving both academic researchers and industry. The machine's architecture includes CPUs, GPUs and FPGAs. It is designed to be energy-efficient, utilizing technologies to reduce power consumption and lower its environmental footprint.</p> <p>Meluxina's FPGA nodes have two 2 Intel Stratix 10MX 16 GB FPGA cards. The FPGA partition contains 20 nodes.</p> <p></p>"},{"location":"meluxina/#connecting-to-meluxina","title":"Connecting to Meluxina","text":"<p>You should have received your credentials some days before the workshop. If not, please contact the organizer. Please follow the LuxProvide documentation to setup your access.</p>"},{"location":"meluxina/#accessing-the-fpga-partition","title":"Accessing the fpga partition","text":"<p>The oneAPI toolkit is evolving quite rapidely. In order to show the last features, the last version has been compiled for the course/workshop.</p>"},{"location":"meluxina/#offline-compilation","title":"Offline compilation","text":"<p>In order to use the oneAPI toolkit, you first need to take an interactive job. We strongly recommend to open a persistent terminal session using <code>tmux</code></p> <p>As we are focusing on FPGA, we select the fpga partition:</p> <p><pre><code>$ tmux new -s fpga_session\n[fpga_session]$ salloc -A p200117 -t 01:00:00 -q default -p fpga -N 1\n</code></pre> Once the fpga node has been allocated to you, you can load the lastest oneAPI toolkit with  <pre><code>[fpga_session]$ module use /project/home/p200117/apps/u100057/easybuild/modules/all\n[fpga_session]$ module load intel-compilers/2023.2.1-fpga_compile\n</code></pre> The root folder containing binaries, include and lib can be located using the <code>${EBROOTINTELMINCOMPILERS}</code>. <pre><code>$ ls ${EBROOTINTELMINCOMPILERS}\ntotal 84\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 compiler\ndrwxr-xr-x 6 apps hpcusers  4096 Nov 12  2022 conda_channel\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 debugger\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 dev-utilities\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 easybuild\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 etc\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 licensing\ndrwxr-xr-x 2 apps hpcusers  4096 Nov 12  2022 logs\n-rwxr-xr-x 1 apps hpcusers 10249 Feb 16  2022 modulefiles-setup.sh\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 mpi\n-rwxr-xr-x 1 apps hpcusers 26718 Feb 11  2022 setvars.sh\n-rw-r--r-- 1 apps hpcusers   155 Sep 13  2019 support.txt\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 tbb\n</code></pre> The process of creating an FPGA hardware image necessitates the use of Intel\u00ae Quartus\u00ae Prime software to transform your design from RTL (Register Transfer Level) to the FPGA's primitive hardware resources. A FPGA BSP (Board Support Package) refers to a collection of software and hardware components designed to support and enable the development of applications on a specific FPGA board. BSPs are commonly used in FPGA development to provide an abstraction layer between the hardware and software, making it easier for developers to create and run their applications on the FPGA platform. In order work with the Bittware Stratix 520MX provided by LuxProvide, we need to load the corresponding BSP.</p> <pre><code>$ module load 520nmx/20.4\n</code></pre>"},{"location":"meluxina/#execution","title":"Execution","text":"<p>Execution does not require the hardware compiler Quartus Prime. In order to execute the FPGA binary once it has been generated, the following modules should be loaded instead:</p> <pre><code>[fpga_session]$ module purge\n[fpga_session]$ module load 520nmx/20.4\n# Don't forget to uncomment the next command if you have started a new tmux/screen session\n#[fpga_session]$ module use /project/home/p200117/apps/u100057/easybuild/modules/all\n[fpga_session]$ module load intel-compilers/2023.2.1-fpga_execute\n</code></pre>"},{"location":"meluxina/#emulation","title":"Emulation","text":"<p>Emulation only relies on the host cpu. In theory, you don't need to load the BSP board but you still need to <code>module load intel-compilers/2023.2.1-fpga_execute</code>.</p>"},{"location":"meluxina/#graphical-sessions-vnc","title":"Graphical sessions (VNC)","text":"<p>During this course/workshop, we will need GUI software and, more particularly,the Intel\u00ae VTune\u2122 Profiler. Using the Intel\u00ae VTune\u2122 Profiler allows you to analyze the performance of your application and identify performance bottlenecks in your code. </p> <p>You can use the profiler in interactive or batch mode. By default, the <code>vtune-gui</code> executable is started after having enabled X11 forwarding (see Meluxina's documentation). X11 forwarding can be very laggy when GUI applications are too demanding. This is the reason why we choose to use VNC.</p> <p>VNC (Virtual Network Computing) is a technology that allows you to remotely access and control a computer's desktop environment over a network connection. It enables users to view and interact with a remote computer as if they were sitting in front of it physically. VNC works by transmitting the graphical user interface (GUI) of the remote computer to the client device and relaying back user input from the client to the remote computer.</p> <p>The VNC system consists of two main components:</p> <ul> <li> <p>VNC Server: This software component runs on the computer that you want to access remotely. It captures the graphical output of the desktop environment and sends it over the network to the client.</p> </li> <li> <p>VNC Viewer: The VNC Viewer is the client software that runs on the device from which you want to access the remote computer. It receives the graphical data from the VNC server and presents it to the user, allowing them to interact with the remote desktop as if it were running locally.</p> </li> </ul> <p>Meluxina nodes do not propose a VNC server application but we can remedy to it using Singularity-CE containers. As this is not a course on container, we will not explain in details the singularity definition file.</p> <p>In order to faciliate its usage, the following launcher can be used to start a batch job.</p> <p>launcher_vtune.sh</p> <pre><code>\n</code></pre> <ul> <li>Line 12 loads singularity</li> <li>Lines 15-17 defines all ports</li> <li>Lines 18 and 19 just echo the commands you will need to execute locally on your laptop<ul> <li>Line 18: Open a ssh tunnel</li> <li>Line 19: The link to access to the web-based GUI</li> </ul> </li> <li>You can adapt the number of task and the number of cores per tasks</li> </ul> <p>Before using this script, it is more convenient to setup a password in interactive mode. So connect to an interactive job using <code>salloc -A &lt;account&gt; -t 00:30:00 -q default -p cpu -N 1</code>. Load the Singularity module and exec vncpasswd using the container <code>vtune_vnc_rocky8.app</code>. </p> <p>Example</p> <pre><code>$ salloc -A p200117 -t 48:00:00 -q default -p fpga -N 1\n(mel3013)$ module load Singularity-CE/3.10.2-GCCcore-11.3.0\n(mel3013)$ singularity exec vnc-rocky8.app vncpasswd\n           INFO:    Converting SIF file to temporary sandbox...\n           Password:\n</code></pre> <p>Add a password and close the interactive job <code>&lt;CTRL-D&gt;</code>. Your vnc password is now setup.</p> <p>Use <code>sbatch launcher_vnc-rocky8.sh</code> to start the vnc server inside a batch job. Once resources are available, the job starts.</p> <pre><code>$ squeue\nJOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n409979       cpu launcher  u100057    p200117  RUNNING       0:08   1-16:00:00      1 mel0533\n409965      fpga interact  u100057    p200117  RUNNING      47:07   2-00:00:00      1 mel3009\n</code></pre> <ul> <li> <p>Record the jobid and open the slurm output file: <pre><code>cat slurm-409979.out \nOn your laptop: ssh -p 8822 -NL 9910:10.3.25.25:8020 u100057@login.lxp.lu \nOpen following link http://localhost:9910/vnc.html\nINFO:    Converting SIF file to temporary sandbox...\nWebSocket server settings:\n  - Listen on :8020\n  - Web server. Web root: /opt/noVNC\n  - No SSL/TLS support (no cert file)\n  - proxying from :8020 to 10.3.25.25:5910\n</code></pre></p> </li> <li> <p>Copy-paste into your laptop terminal the ssh command</p> </li> <li>Then click on the http link. You should see a new tab opening with the following content:</li> </ul> <p></p> <ul> <li>Click on connect and enter your vnc password</li> <li>Your should see now the VTune profiler</li> </ul> <p></p>"},{"location":"optimization/","title":"Optimizing SYCL programs for Intel\u00ae FPGA cards","text":"<p>Optimizing SYCL code for Intel FPGAs requires a combination of understanding the FPGA hardware, the SYCL programming model, and the specific compiler features provided by Intel. Here are some general guidelines to optimize Intel FPGA SYCL code.</p> <p>Compared to OpenCL, the Intel\u00ae oneAPI DPC++ compiler has enhanced features to detect possible optimizations( vectorization, static coalescing, etc ...). Nonetheless, some rules need to be followed to make sure the compiler is able to apply these optimizations. </p> <p>Optimizing your design</p> <p>As this course/workshop is only an introduction to the Intel\u00ae oneAPI for FPGA programming, we can't unfortunately provide all existing and possible optimizations. Many more optimizations can be found in the Intel official documentation.</p>"},{"location":"optimization/#loop-optimization","title":"Loop optimization","text":"<p>Loop unrolling is an optimization technique that aims to increase parallelism and, consequently, the throughput of certain computational tasks, particularly when implemented in hardware environments such as FPGAs. </p> <ol> <li> <p>Pipelining Synergy: Loop unrolling often goes hand in hand with pipelining in FPGAs. When loops are unrolled, each unrolled iteration can be pipelined, leading to even greater throughput enhancements.</p> </li> <li> <p>Resource Utilization: While loop unrolling can significantly speed up operations, it also consumes more FPGA resources, like Logic Elements (LEs) and registers, because of the duplicated hardware. Hence, there's a trade-off between speed and resource utilization.</p> </li> <li> <p>Memory Access: Unrolling loops that involve memory operations can lead to increased memory bandwidth utilization. In cases where memory bandwidth is a bottleneck, unrolling can provide substantial performance improvements.</p> </li> <li> <p>Latency &amp; Throughput: Loop unrolling doesn't necessarily reduce the latency of a single loop iteration (the time taken for one iteration to complete), but it can significantly improve the throughput (number of completed operations per unit time).</p> </li> <li> <p>Reduction in Control Logic: Unrolling can reduce the overhead associated with the loop control logic, such as incrementing the loop counter and checking the loop termination condition.</p> <p> Loop Optimization in HLS </p> </li> <li> <p>Unrolling loops will help to reduce the Initialization Interval (II) as you can notice on the previous figure.</p> </li> </ol> <p>Increasing throughput with loop unrolling</p> How to unroll loopsQuestionSolution <ul> <li>Unrolling loop can be done using the <code>#pragma unroll &lt;N&gt;</code></li> <li><code>&lt;N&gt;</code> is the unroll factor</li> <li><code>#pragma unroll 1</code> : prevent a loop in your kernel from unrolling</li> <li><code>#pragma unroll</code> : let the offline compiler decide how to unroll the loop  <pre><code>handler.single_task&lt;class example&gt;([=]() {\n    #pragma unroll\n        for (int i = 0; i &lt; 10; i++) {\n            acc_data[i] += i;\n        }\n    #pragma unroll 1\n    for (int k = 0; k &lt; N; k++) {\n        #pragma unroll 5\n        for (int j = 0; j &lt; N; j++) {\n            acc_data[j] = j + k;\n        }\n    }\n});\n</code></pre></li> </ul> <ul> <li>Consider the following code that you can find at <code>oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/Features/loop_unroll</code></li> <li>Note that Intel did not consider data alignment which could impact performance</li> <li>We included <code>#include &lt;boost/align/aligned_allocator.hpp&gt;</code> to create aligned std::vector</li> <li>The following SYCL code has been already compiled for you, execute it on the FPGA nodes for several data input size and record the throughput and kernel time</li> <li>What do you observe ? <pre><code>//==============================================================\n// Copyright Intel Corporation\n//\n// SPDX-License-Identifier: MIT\n// =============================================================\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\nusing namespace sycl;\n\nusing aligned64_vector= std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt;;\n\n// Forward declare the kernel name in the global scope.\n// This FPGA best practice reduces name mangling in the optimization reports.\ntemplate &lt;int unroll_factor&gt; class VAdd;\n\n// This function instantiates the vector add kernel, which contains\n// a loop that adds up the two summand arrays and stores the result\n// into sum. This loop will be unrolled by the specified unroll_factor.\ntemplate &lt;int unroll_factor&gt;\nvoid VecAdd(const aligned64_vector &amp;summands1,\n            const aligned64_vector &amp;summands2, aligned64_vector &amp;sum,\n            size_t array_size) {\n\n#if FPGA_SIMULATOR\n  auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n  auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n  auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n  try {\n    queue q(selector,property::queue::enable_profiling{});\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    buffer buffer_summands1(summands1);\n    buffer buffer_summands2(summands2);\n    buffer buffer_sum(sum);\n\n    event e = q.submit([&amp;](handler &amp;h) {\n      accessor acc_summands1(buffer_summands1, h, read_only);\n      accessor acc_summands2(buffer_summands2, h, read_only);\n      accessor acc_sum(buffer_sum, h, write_only, no_init);\n\n      h.single_task&lt;VAdd&lt;unroll_factor&gt;&gt;([=]()\n                                         [[intel::kernel_args_restrict]] {\n        // Unroll the loop fully or partially, depending on unroll_factor\n        #pragma unroll unroll_factor\n        for (size_t i = 0; i &lt; array_size; i++) {\n          acc_sum[i] = acc_summands1[i] + acc_summands2[i];\n        }\n      });\n    });\n\n    double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();\n    double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();\n    // convert from nanoseconds to ms\n    double kernel_time = (double)(end - start) * 1e-6;\n\n    std::cout &lt;&lt; \"unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \" kernel time : \" &lt;&lt; kernel_time &lt;&lt; \" ms\\n\";\n    std::cout &lt;&lt; \"Throughput for kernel with unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \": \";\n    std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(3)\n#if defined(FPGA_SIMULATOR)\n              &lt;&lt; ((double)array_size / kernel_time) / 1e3f &lt;&lt; \" MFlops\\n\";\n#else\n              &lt;&lt; ((double)array_size / kernel_time) / 1e6f &lt;&lt; \" GFlops\\n\";\n#endif\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n}\n\nint main(int argc, char *argv[]) {\n#if defined(FPGA_SIMULATOR)\n  size_t array_size = 1 &lt;&lt; 4;\n#else\n  size_t array_size = 1 &lt;&lt; 26;\n#endif\n\n  if (argc &gt; 1) {\n    std::string option(argv[1]);\n    if (option == \"-h\" || option == \"--help\") {\n      std::cout &lt;&lt; \"Usage: \\n&lt;executable&gt; &lt;data size&gt;\\n\\nFAILED\\n\";\n      return 1;\n    } else {\n      array_size = std::stoi(option);\n    }\n  }\n\n  aligned64_vector summands1(array_size);\n  aligned64_vector summands2(array_size);\n\n  aligned64_vector sum_unrollx1(array_size);\n  aligned64_vector sum_unrollx2(array_size);\n  aligned64_vector sum_unrollx4(array_size);\n  aligned64_vector sum_unrollx8(array_size);\n  aligned64_vector sum_unrollx16(array_size);\n\n  // Initialize the two summand arrays (arrays to be added to each other) to\n  // 1:N and N:1, so that the sum of all elements is N + 1\n  for (size_t i = 0; i &lt; array_size; i++) {\n    summands1[i] = static_cast&lt;float&gt;(i + 1);\n    summands2[i] = static_cast&lt;float&gt;(array_size - i);\n  }\n\n  std::cout &lt;&lt; \"Input Array Size:  \" &lt;&lt; array_size &lt;&lt; \"\\n\";\n\n  // Instantiate VecAdd kernel with different unroll factors: 1, 2, 4, 8, 16\n  // The VecAdd kernel contains a loop that adds up the two summand arrays.\n  // This loop will be unrolled by the specified unroll factor.\n  // The sum array is expected to be identical, regardless of the unroll factor.\n  VecAdd&lt;1&gt;(summands1, summands2, sum_unrollx1, array_size);\n  VecAdd&lt;2&gt;(summands1, summands2, sum_unrollx2, array_size);\n  VecAdd&lt;4&gt;(summands1, summands2, sum_unrollx4, array_size);\n  VecAdd&lt;8&gt;(summands1, summands2, sum_unrollx8, array_size);\n  VecAdd&lt;16&gt;(summands1, summands2, sum_unrollx16, array_size);\n\n  // Verify that the output data is the same for every unroll factor\n  for (size_t i = 0; i &lt; array_size; i++) {\n    if (sum_unrollx1[i] != summands1[i] + summands2[i] ||\n        sum_unrollx1[i] != sum_unrollx2[i] ||\n        sum_unrollx1[i] != sum_unrollx4[i] ||\n        sum_unrollx1[i] != sum_unrollx8[i] ||\n        sum_unrollx1[i] != sum_unrollx16[i]) {\n      std::cout &lt;&lt; \"FAILED: The results are incorrect\\n\";\n      return 1;\n    }\n  }\n  std::cout &lt;&lt; \"PASSED: The results are correct\\n\";\n  return 0;\n}\n</code></pre></li> </ul> Unroll factor kernel execution time (ms) Throughput (GFlops) 1 77 0.447 2 58 0.591 4 43 0.804 8 40 0.857 16 39 0.882 <ul> <li>Increasing the unroll factor improves throughput    </li> <li>Nonetheless, unrolling large loops should be avoided as it would require a large amount of hardware</li> </ul> <p>Recording kernel time</p> <ul> <li>In this example, we have also seen how to record kernel time.</li> <li>Using the property `property::queue::enable_profiling{}`` adds the requirement that the runtime must capture profiling information for the command groups that are submitted from the queue </li> <li>You can the capture  the start &amp; end time using the following two commands:<ul> <li><code>double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();</code></li> <li><code>double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();</code></li> </ul> </li> </ul> <p>Caution with nested loops</p> <ul> <li>Loop unrolling involves replicating the hardware of a loop body multiple times and reducing the trip count of a loop. Unroll loops to reduce or eliminate loop control overhead on the FPGA. </li> <li>Loop-unrolling can be used to eliminate nested-loop structures.</li> <li>However avoid unrolling the outer-loop which will lead to Resource Exhaustion and dramatically increase offline compilation</li> </ul>"},{"location":"optimization/#simd-work-items-for-nd-range-kernels","title":"SIMD Work Items for ND-Range kernels","text":"<ul> <li> <p>In the Reporting &amp; Profiling section we have seen that vectorization can improve bandwidth</p> </li> <li> <p>ND-range kernel should use instead of classical data-parallel kernels</p> </li> <li>The work-group size needs to be set using the attribute <code>[[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</code></li> <li>To specify the number of SIMD work_items, you will need to add the following attribute <code>[[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]]</code></li> <li>Note that NUM_SIMD_WORK_ITEMS should divide evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> </ul> <p>Example</p> <pre><code>```cpp linenums=\"1\"\n...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        \n    [=](sycl::nd_item&lt;1&gt; it) \n    [[intel::num_simd_work_items(8),\n    sycl::reqd_work_group_size(1, 1, 128)]] {\n    auto gid = it.get_global_id(0);\n    accessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n    });\n});\n...\n```\n\n* The **128** work-items are evenly distributed among **8** SIMD lanes\n* $\\frac{128}{8}$ = 16 wide vector operation\n* The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies\n</code></pre>"},{"location":"optimization/#loop-coalescing","title":"Loop coalescing","text":"<p>Utilize the <code>loop_coalesce</code> attribute to instruct the Intel\u00ae oneAPI DPC++/C++ Compiler to merge nested loops into one, preserving the loop's original functionality. By coalescing loops, you can minimize the kernel's area consumption by guiding the compiler to lessen the overhead associated with loop management.</p> <p>Coalesced two loops</p> Using the loop_coalesce attribute <pre><code>[[intel::loop_coalesce(2)]]\nfor (int i = 0; i &lt; N; i++)\n   for (int j = 0; j &lt; M; j++)\n      sum[i][j] += i+j;\n</code></pre> Equivalent code <pre><code>int i = 0;\nint j = 0;\nwhile(i &lt; N){\n  sum[i][j] += i+j;\n  j++;\n  if (j == M){\n    j = 0;\n    i++;\n  }\n}\n</code></pre>"},{"location":"optimization/#ignore-loop-carried-dependencies","title":"Ignore Loop-carried dependencies","text":"<p>The ivdep attribute in Intel's oneAPI (as well as in other Intel compiler tools) is used to give a hint to the compiler about the independence of iterations in a loop. This hint suggests that there are no loop-carried memory dependencies that the compiler needs to account for when attempting to vectorize or parallelize the loop.</p> <p>When you use ivdep, you're essentially telling the compiler: \"Trust me, I've reviewed the code, and the iterations of this loop do not have dependencies on each other. So, you can safely vectorize or parallelize this loop for better performance.\"</p> <p>ivdep attribute</p> <pre><code>#pragma ivdep\nfor (int i = 1; i &lt; N; i++) {\n    A[i] = A[i - 1] + B[i];\n}\n</code></pre> <p>Caution</p> <p>You should be very careful when using ivdep. Incorrectly using this pragma on a loop that does have dependencies can lead to unexpected results or undefined behavior. Always ensure that there are truly no dependencies in the loop before applying this hint.</p>"},{"location":"optimization/#memory","title":"Memory","text":""},{"location":"optimization/#static-coalescing","title":"Static coalescing","text":"<ul> <li> <p>Static coalescing is performed by the Intel\u00ae oneAPI DPC++/C++ Compiler contiguous accesses to global memory can be merged into a single wide access.</p> </li> <li> <p>For static memory coalescing to occur, your code should be structured so that the compiler can detect a linear access pattern at compile time. The initial kernel code depicted in the previous figure can leverage static memory coalescing, as all indices into buffers a and b increase with offsets recognizable during compilation.</p> </li> </ul> <p> </p> FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits -- Figure 17-21"},{"location":"optimization/#data-structure-alignment","title":"Data structure alignment","text":"<p>In order to performance, structure alignment can be modified to be properly aligned. By default, the offline compiler aligns these elements based on:</p> <ul> <li>The alignment should be a power of two.</li> <li>The alignment should be a multiple of the least common multiple (LCM) of the word-widths of the structure member sizes.</li> </ul> <p>Let's take a simple but clear example to understand why alignment is so important.</p> <p></p> <p>Removing padding and changing structure alignment</p> CodeExecution time <ul> <li> <p>The following code show the impact of changing the alignmement and padding using three scenarii:</p> <ul> <li> <p>Default alignment and padding </p> </li> <li> <p>Removing padding</p> </li> <li> <p>Changing alignment </p> </li> </ul> </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;chrono&gt;\nusing namespace std::chrono;\n\n#define ALIGNMENT 64\n#define IT 1024\n\n\nconstexpr int kVectSize = 2048;\n\n\ntemplate&lt;typename T&gt;\nvoid test_structure( T* device,sycl::queue &amp;q, int nb_iters){\n\n      sycl::event e;\n      const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n\n      auto start = high_resolution_clock::now();\n      sycl::buffer buffer_device{device, sycl::range(kVectSize),props};\n      e = q.submit([&amp;](sycl::handler &amp;h) {\n       sycl::accessor accessor_device{buffer_device, h, sycl::read_write};\n       h.single_task([=]() {\n       for(int it=0;it &lt; nb_iters ;it++){\n        for (int idx = 0; idx &lt; kVectSize; idx++) {\n          accessor_device[idx].C = (int)accessor_device[idx].A + accessor_device[idx].B;\n         }\n        }\n        });\n       });\n\n    sycl::host_accessor buffer_host(buffer_device);\n    auto stop = high_resolution_clock::now();\n    // convert from nanoseconds to ms\n    duration&lt;double&gt; kernel_time = stop - start;\n\n    std::cout  &lt;&lt; \" Time (\" &lt;&lt;typeid(T).name()&lt;&lt;  \") : \" &lt;&lt; kernel_time.count() &lt;&lt; \" ms\\n\";\n}\n\nint main() {\n\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector,sycl::property::queue::enable_profiling{});\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    typedef struct {\n        char A;\n        int  B;\n        int  C;\n    } mystruct;\n\n    typedef struct __attribute__ ((packed)) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed;\n\n\n    typedef struct __attribute__ ((packed)) __attribute__ ((aligned(16))) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed_aligned;\n\n    //mystruct host_vec_a[kVectSize];\n    //mystruct_packed host_vec_b[kVectSize];\n    //mystruct_packed_aligned host_vec_c[kVectSize];\n\n    mystruct* vec_a = new(std::align_val_t{ 64 }) mystruct[kVectSize];\n    mystruct_packed* vec_b = new(std::align_val_t{ 64 }) mystruct_packed[kVectSize];\n    mystruct_packed_aligned* vec_c = new(std::align_val_t{ 64 }) mystruct_packed_aligned[kVectSize];\n\n\n    //mystruct * vec_a = static_cast&lt;mystruct*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct),q));\n    //mystruct_packed*vec_b = static_cast&lt;mystruct_packed*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct_packed),q));\n    //mystruct_packed_aligned*vec_c = static_cast&lt;mystruct_packed_aligned*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct_packed_aligned),q));\n\n    for (int i = 0; i &lt; kVectSize; i++) {\n        vec_a[i].A = vec_b[i].A = vec_c[i].A = char(std::rand() % 256);\n        vec_a[i].B = vec_b[i].B = vec_c[i].B = std::rand();\n        vec_a[i].C = vec_b[i].C = vec_c[i].C = std::rand();\n    }\n\n    std::cout &lt;&lt; \"Packed with default alignment\" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    test_structure&lt;mystruct&gt;(vec_a,q,IT);\n    test_structure&lt;mystruct_packed&gt;(vec_b,q,IT);\n    test_structure&lt;mystruct_packed_aligned&gt;(vec_c,q,IT);\n\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n\n    //sycl::free(vec_a,q);\n    //sycl::free(vec_b,q);\n    //sycl::free(vec_c,q);\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> Scenario Processing time (seconds) Default alignment and padding 14.33 Removing padding 6.35 Changing alignment 0.03"},{"location":"optimization/#local-memory","title":"Local memory","text":""},{"location":"optimization/#local-memory-in-nd-range-kernels","title":"Local memory in ND-Range kernels","text":"<ul> <li>You can improve memory access by using local and private memory.</li> <li>When you define a private array, group local memory, or a local accessor, the Intel\u00ae oneAPI DPC++/C++ Compiler generates kernel memory in the hardware. This kernel memory is often termed on-chip memory since it originates from memory resources, like RAM blocks, present on the FPGA.</li> <li>Local or private memory is a fast memory that should be favored when resources allow.</li> </ul> <p>Private memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n// Create an accessor for device global memory from buffer buff\naccessor acc(buff, h, write_only);\ncgh.single_task([=]() {\n     // Declare a private array\n     int T[N];\n     // Write to private memory\n     for (int i = 0; i &lt; N; i++)\n        T[i] = i;\n     // Read from private memory and write to global memory through the accessor\n     for (int i = 0; i &lt; N; i+=2)\n        acc[i] = T[i] + T[i+1];\n     });\n}); \n...\n</code></pre> <ul> <li>To set aside local memory that can be accessed and shared by every work item within a workgroup, establish a group-local variable within the function scope of a workgroup. Do this using the group_local_memory_for_overwrite function, illustrated in the subsequent example:</li> </ul> <p>Local memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n    h.parallel_for(\n        nd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item) {\n        int local_id = item.get_local_id();\n        auto ptr = group_local_memory_for_overwrite&lt;int[16]&gt;(item.get_group());\n        auto&amp; ref = *ptr;\n        ref[local_id] = local_id++ ;\n        });\n    });\n... \n</code></pre> <ul> <li>The ND-Range kernel has 16 workgroups with 16 work items for each group.</li> <li>A group-local variable (int[16]) is created for each group and shared through a multi_ptr to all work-items of the same group</li> </ul>"},{"location":"optimization/#settings-memory-banks","title":"Settings memory banks","text":"<ul> <li>Local data can be stored  in separate  local memory banks for parallel memory accesses</li> <li>Number of banks of a local memory can be adjusted (e.g., to increase the parallel access) </li> <li>Add the following attributes <code>[[intel::numbanks(#NB), intel::bankwidth(#BW)]]</code>:  <ul> <li><code>#NB</code> : number of banks </li> <li><code>#BW</code>: bankwidth to be considered </li> </ul> </li> <li>Ex: <code>[[intel::numbanks(8), intel::bankwidth(16)]]lmem[8][4]</code>; <ul> <li>No two element can be accessed in parallel in lmem </li> <li>Single bank local memory </li> </ul> </li> <li>All rows accessible in parallel with numbanks(8) </li> <li>Different configurations patterns can be adopted </li> </ul> <p>Masking the last index</p> <ul> <li>Intel's documentation states that \"To enable parallel access, you must mask the dynamic access on the lower array index\" <pre><code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4];\n#pragma unroll\nfor (int i = 0; i &lt; 4; i+=2) {\n    lmem[i][x &amp; 0x3] = ...;\n} \n</code></pre></li> </ul> <p>Exercice</p> QuestionSolution <ul> <li>Could you briefly describe the bank configuration of the following local memory declaration; <pre><code> [[intel::numbanks(4),intel::bankwidth(8)]] int lmem[2][4];\n</code></pre></li> </ul> <p></p>"},{"location":"optimization/#local-memory-replication","title":"Local memory replication","text":"<p>Example</p> <p> <pre><code>[[intel::fpga_memory,\nintel::singlepump,\nintel::max_replicates(3)]] int lmem[16]; \nlmem[waddr] = lmem[raddr] +\n              lmem[raddr + 1] +\n              lmem[raddr + 2]; \n</code></pre> <ul> <li>The offline compiler can replicate the local memory</li> <li>This allows to create multiple ports </li> <li>Behaviour: <ul> <li>All read ports will be accessed in parallel </li> <li>All write ports are connected together</li> <li>Data between replicate is identical </li> </ul> </li> <li>Parallel access to all ports is possible but consumes more hardware resources</li> <li><code>[[intel::max_replicates(N)]]</code> control the replication factor</li> </ul> <p> </p>"},{"location":"optimization/#task-parallelism-with-inter-kernel-pipes","title":"Task parallelism with Inter-Kernel Pipes","text":"<p>Pipes function as a first-come, first-served buffer system, linking different parts of a design. The Intel\u00ae oneAPI DPC++/C++ Compiler offers various pipe types:</p> <ul> <li> <p>Host Pipes: These establish a connection between a host and a device.</p> </li> <li> <p>Inter-Kernel Pipes: These facilitate efficient and low-latency data transfer and synchronization between kernels. They enable kernels to interact directly using on-device FIFO buffers, which utilize FPGA memory. The Intel\u00ae oneAPI DPC++/C++ Compiler promotes simultaneous kernel operation. By employing inter-kernel pipes for data movement among concurrently running kernels, data can be transferred without waiting for a kernel to finish, enhancing your design's throughput.</p> </li> <li> <p>I/O Pipes: This is a one-way connection to the hardware, either as a source or sink, which can be linked to an FPGA board's input or output functionalities. Such functionalities could encompass network interfaces, PCIe\u00ae, cameras, or other data acquisition or processing tools and protocols.</p> </li> </ul>"},{"location":"optimization/#inter-kernel-pipes","title":"Inter-Kernel Pipes","text":"<ul> <li>We will only focus on Inter-Kernel Pipes to leverage task parallelism</li> <li>As for OpenCL programming, pipes can be blocking or non-blocking</li> <li>For Intel\u00ae oneAPI with FPGA, you need to include FPGA extension: <pre><code>#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n</code></pre></li> </ul> <p>Pipe creation and usage</p> Blocking pipesNon-Blocking pipes <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i]); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        valid_write = false;\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i],valid_write); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        valid_read = false;\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(valid_read); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <p>Stalling pipes</p> <ul> <li>Care should be taken when implementing pipes, especially when there is a strong imbalance between the consumer kernel reading from the pipe and the producer kernel that feed the pipe. </li> <li>Stalling pipes can be disastrous when using blocking pipes</li> </ul>"},{"location":"qcfpga/","title":"QES with QCFPGA on JupyterLab","text":"In\u00a0[1]: Copied! <pre>%%bash\nKERNEL=\"$HOME/.local/share/jupyter/kernels/qcfpga\"\nmkdir -p $KERNEL\nPRELOAD=\"$KERNEL/start.sh\"\nJSON=\"$KERNEL/kernel.json\"\ncat &lt;&lt; 'EOF' &gt; $JSON\n{\n \"argv\": [\n  \"{resource_dir}/start.sh\",\n  \"python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"QCFPGA\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\nEOF\n\ncat &lt;&lt; 'EOF' &gt; $PRELOAD\n#!/bin/bash\nmodule load QCFPGA\nmodule load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nexport LD_PRELOAD=${JEMALLOC_PRELOAD}\nexport PYOPENCL_COMPILER_OUTPUT=1\nexec \"$@\"\nEOF\n\nchmod u+x $PRELOAD\n</pre> %%bash KERNEL=\"$HOME/.local/share/jupyter/kernels/qcfpga\" mkdir -p $KERNEL PRELOAD=\"$KERNEL/start.sh\" JSON=\"$KERNEL/kernel.json\" cat &lt;&lt; 'EOF' &gt; $JSON {  \"argv\": [   \"{resource_dir}/start.sh\",   \"python\",   \"-m\",   \"ipykernel_launcher\",   \"-f\",   \"{connection_file}\"  ],  \"display_name\": \"QCFPGA\",  \"language\": \"python\",  \"metadata\": {   \"debugger\": true  } } EOF  cat &lt;&lt; 'EOF' &gt; $PRELOAD #!/bin/bash module load QCFPGA module load jemalloc export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision) export LD_PRELOAD=${JEMALLOC_PRELOAD} export PYOPENCL_COMPILER_OUTPUT=1 exec \"$@\" EOF  chmod u+x $PRELOAD  <ul> <li>Execute the following cell to reload everything</li> </ul> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 <ul> <li>Now change the current kernel by the new one: <code>Kernel --&gt; Change Kernels --&gt; QCFPGA</code></li> <li>If you see a white circle, the kernel is ready</li> </ul> In\u00a0[3]: Copied! <pre># Import QCFPGA\nimport qcfpga\nimport numpy as np\n\n# Create a new quantum register with 1 qubits\nregister = qcfpga.State(1)\n# Let's try the Hadamard gate\nregister.h(0)\nnp.round(register.probabilities(),2)\n</pre> # Import QCFPGA import qcfpga import numpy as np  # Create a new quantum register with 1 qubits register = qcfpga.State(1) # Let's try the Hadamard gate register.h(0) np.round(register.probabilities(),2) <pre>/apps/USE/easybuild/release/2023.1/software/PyOpenCL/2023.1.4-foss-2023a-ifpgasdk-20.4/lib/python3.11/site-packages/pyopencl/__init__.py:528: CompilerWarning: From-binary build succeeded, but resulted in non-empty logs:\nBuild on &lt;pyopencl.Device 'p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)' on 'Intel(R) FPGA SDK for OpenCL(TM)' at 0x153d920e4898&gt; succeeded, but said:\n\nTrivial build\n  lambda: self._prg.build(options_bytes, devices),\n</pre> Out[3]: <pre>array([0.5, 0.5], dtype=float32)</pre> In\u00a0[4]: Copied! <pre>register = qcfpga.State(2)\n\nregister.h(0) # Applies the Hadamard  gate to the first qubit.\nregister.x(1) # Applies a pauli-x  gate to the second qubit.\nnp.round(register.probabilities(),2)\n</pre> register = qcfpga.State(2)  register.h(0) # Applies the Hadamard  gate to the first qubit. register.x(1) # Applies a pauli-x  gate to the second qubit. np.round(register.probabilities(),2) Out[4]: <pre>array([0. , 0. , 0.5, 0.5], dtype=float32)</pre> <p>These are the gates that can be applied to a register:</p> <ul> <li><p>The Hadamard gate: h - <code>state.h(0)</code></p> </li> <li><p>The S gate: s - <code>state.s(0)</code></p> </li> <li><p>The T gate: t - <code>state.t(0)</code></p> </li> <li><p>The Pauli-X / NOT gate: x - <code>state.x(0)</code></p> </li> <li><p>The Pauli-Y gate: y - <code>state.y(0)</code></p> </li> <li><p>The Pauli-Z gate: z - <code>state.z(0)</code></p> </li> <li><p>The CNOT gate: cx -<code>state.cx(0, 1) # CNOT with control = 0, target = 1</code></p> </li> <li><p>The SWAP gate: swap -<code>state.swap(0,1) # Swaps the 0th and 1st qubit</code></p> </li> <li><p>The Toffoli gate: toffoli -<code>state.toffoli(0, 1, 2) # Toffoli with controls = (0, 1), target = 2</code></p> </li> </ul> In\u00a0[5]: Copied! <pre>''' \nFor example, you can also use any of the gates as controlled gates. \nControlled gates can be also used to entangle state\n'''\nx = qcfpga.gate.x()\nh = qcfpga.gate.h()\n\nregister = qcfpga.State(5)\nregister.apply_gate(h,0)\nregister.apply_controlled_gate(x, 0, 1)\nnp.round(register.probabilities(),2)\n</pre> '''  For example, you can also use any of the gates as controlled gates.  Controlled gates can be also used to entangle state ''' x = qcfpga.gate.x() h = qcfpga.gate.h()  register = qcfpga.State(5) register.apply_gate(h,0) register.apply_controlled_gate(x, 0, 1) np.round(register.probabilities(),2) Out[5]: <pre>array([0.5, 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n       0. , 0. , 0. , 0. , 0. , 0. ], dtype=float32)</pre> In\u00a0[6]: Copied! <pre>'''\nIt is also trivial to apply a gate to all qubit of a register\n'''\nh = qcfpga.gate.h()\n\nregister = qcfpga.State(3)\nregister.apply_all(h)\nnp.round(register.probabilities(),3)\n</pre> ''' It is also trivial to apply a gate to all qubit of a register ''' h = qcfpga.gate.h()  register = qcfpga.State(3) register.apply_all(h) np.round(register.probabilities(),3)  Out[6]: <pre>array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n      dtype=float32)</pre> In\u00a0[7]: Copied! <pre>gate_matrix = np.array([\n    [np.cos(np.pi/6), -np.sin(np.pi/6)],\n    [np.sin(np.pi/6),np.cos(np.pi/6)]\n])\n\ngate = qcfpga.Gate(gate_matrix)\nregister = qcfpga.State(3)\nregister.apply_all(gate)\nnp.round(register.probabilities(),3)\n</pre> gate_matrix = np.array([     [np.cos(np.pi/6), -np.sin(np.pi/6)],     [np.sin(np.pi/6),np.cos(np.pi/6)] ])  gate = qcfpga.Gate(gate_matrix) register = qcfpga.State(3) register.apply_all(gate) np.round(register.probabilities(),3)  Out[7]: <pre>array([0.422, 0.141, 0.141, 0.047, 0.141, 0.047, 0.047, 0.016],\n      dtype=float32)</pre> In\u00a0[8]: Copied! <pre># Create a new quantum register with 2 qubits\nregister = qcfpga.State(2)\n\n# Apply a hadamard (H) gate to the first qubit.\n# You should note that the qubits are zero indexed\nregister.h(0)\n\n# Add a controlled not (CNOT/CX) gate, with the control as\n# the first qubit and target as the second.\n# The register will now be in the bell state.\nregister.cx(0, 1)\n\n# Perform a measurement with 1000 samples\nresults = register.measure(samples=1000)\n\n# Show the results\nprint(results)\n</pre> # Create a new quantum register with 2 qubits register = qcfpga.State(2)  # Apply a hadamard (H) gate to the first qubit. # You should note that the qubits are zero indexed register.h(0)  # Add a controlled not (CNOT/CX) gate, with the control as # the first qubit and target as the second. # The register will now be in the bell state. register.cx(0, 1)  # Perform a measurement with 1000 samples results = register.measure(samples=1000)  # Show the results print(results) <pre>{'00': 536, '11': 464}\n</pre> In\u00a0[9]: Copied! <pre>import qcfpga\n\nnum_qubits = 20 # The number of qubits to use\na = 70 # The hidden integer, bitstring is 1100101\n\nregister = qcfpga.State(num_qubits) # Create a new quantum register\n\nregister.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit\n\n# Apply the inner products oracle\nfor i in range(num_qubits):\n    if a &amp; (1 &lt;&lt; i) != 0:\n        register.z(i)\nregister.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit\n\nresults = register.measure(samples=1000) # Measure the register (sample 1000 times)\nprint(results)\n</pre> import qcfpga  num_qubits = 20 # The number of qubits to use a = 70 # The hidden integer, bitstring is 1100101  register = qcfpga.State(num_qubits) # Create a new quantum register  register.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit  # Apply the inner products oracle for i in range(num_qubits):     if a &amp; (1 &lt;&lt; i) != 0:         register.z(i) register.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit  results = register.measure(samples=1000) # Measure the register (sample 1000 times) print(results) <pre>{'00000000000001000110': 1000}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"qcfpga/#connecting-to-the-jlab-portal","title":"Connecting to the jlab portal\u00b6","text":"<ol> <li><p>Please connect to the jlab portal first.</p> </li> <li><p>Start a server. Don't forget to select FPGA instead of CPU (see below):</p> </li> </ol> <ol> <li><p>Clone the training repository: <code>git clone https://github.com/LuxProvide/QuantumFPGA</code></p> </li> <li><p>Finally, open the notebook <code>qcfpa.ipynb</code> in JupyterLab.</p> </li> </ol>"},{"location":"qcfpga/#setting-the-ipython-kernel","title":"Setting the IPython kernel\u00b6","text":"<p>We need now to create a dedicated IPython kernel to be able to run efficiently on efficiently on FPGA node</p> <ul> <li><p>Kernels are by default located in this folder <code>$HOME/.local/share/jupyter/kernels</code></p> </li> <li><p>Execute the following cell to create a custom kernel named QCFPGA</p> </li> </ul>"},{"location":"qcfpga/#the-qcfpga-library","title":"The QCFPGA library\u00b6","text":"<p>QCFPGA is a software library which is a fork of the public QCGPU software that was designed to perform quantum computing simulations on graphics processing units (GPUs) using PyOpenCL. The main idea behind QCFPGA is to utilize the parallel processing capabilities of modern FPGAs to speed up quantum simulations, which are computationally intensive tasks that can benefit greatly from the pipeline parallelism offered by modern FPGAs.</p> <p>The library provides a high-level interface for defining quantum states, applying gates, and performing measurements, much like other quantum computing frameworks. Nonetheless, the library is far from being complete as the Qiskit (IBM) or Cirq (Google).</p> <p>QFPGA was adapted from QCGPU as a proof of concept with the intent to make quantum computing simulations more accessible and faster, leveraging the powerful computational capabilities of FPGAs to handle state vector manipulations typical in quantum computing.</p>  \u26a0\ufe0f  QCFPGA is a Work In Progress and may be subject to changes in the near future. Our main goal is to take advantage of kernel optimization on FPGAs and develop a multi-node version.For any problem, please contact the support team using our  servicedesk portal"},{"location":"qcfpga/#built-in-gates","title":"Built-In Gates\u00b6","text":"<p>In Quantum Computing, gates are used to manipulate quantum registers and to implement quantum algorithms.</p> <p>There are a number of gates built into QCGPU and QCFPGA. They can all be applied the same way:</p>"},{"location":"qcfpga/#applying-a-gate-to-all-qubits-in-parallel","title":"Applying a gate to all qubits in parallel\u00b6","text":""},{"location":"qcfpga/#define-your-own-gate","title":"Define your own gate\u00b6","text":"<ul> <li><p>Custom gates in QCFPGA use the <code>qcfpga.Gate</code> class.</p> </li> <li><p>Only single gate qubits can be defined</p> </li> </ul>  \u26a0\ufe0f  The input to the `Gate` constructor is checked to be a 2x2 unitary matrix."},{"location":"qcfpga/#mesuring-register","title":"Mesuring register\u00b6","text":"<ul> <li>Normally, real qubits will collapse, i.e., become classical qubits after measuring the register</li> <li>For obvious reason, it would require to rebuild a new circuit and repeat the experience</li> </ul>"},{"location":"qcfpga/#use-case-bernstein-vazirani-algorithm","title":"Use case: Bernstein-Vazirani Algorithm\u00b6","text":"<p>The Bernstein-Vazirani algorithm is a quantum algorithm that highlights the superiority of quantum computers in solving specific problems more efficiently than classical computers. This algorithm solves the problem of determining a hidden binary string with minimal queries to a given function.</p>"},{"location":"qcfpga/#problem-setup","title":"Problem Setup\u00b6","text":"<p>You are given a black box function (oracle) that computes:</p> <ul> <li>Function: $ f(x) = a \\cdot x $<ul> <li>a is a hidden string of $ n $ bits.</li> <li>x is an  $n$-bit string.</li> <li>The dot product $a \\cdot x $ is calculated as $ (a_1x_1 + a_2x_2 + \\dots + a_nx_n) $ modulo 2.</li> </ul> </li> <li>Goal: Determine the hidden string $a $ using the fewest number of queries to $f$.</li> </ul>"},{"location":"qcfpga/#classical-approach","title":"Classical Approach\u00b6","text":"<p>Classically, you would need to make $n$ queries to the oracle, each with x set to vectors representing each bit position (e.g., $ 100...0, 010...0, \\ldots, 000...1 $), revealing one bit of $ a $ per query.</p>"},{"location":"qcfpga/#quantum-solution","title":"Quantum Solution\u00b6","text":"<p>The Bernstein-Vazirani algorithm uses a quantum computer to identify $ a $ with a single query, showing an exponential improvement in query complexity.</p>"},{"location":"qcfpga/#steps-of-the-algorithm","title":"Steps of the Algorithm:\u00b6","text":"<ol> <li><p>Initialization: Start with $ n $ qubits in the state $ |0\\rangle $ and one auxiliary qubit in the state $|1\\rangle $.</p> </li> <li><p>Apply Hadamard Gates: Apply Hadamard gates to all qubits, transforming each $ |0\\rangle $to $ \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} $ and $ |1\\rangle $ to $\\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}$.</p> </li> <li><p>Query the Oracle: The function $ f(x) $ modifies the auxiliary qubit by $ (-1)^{f(x)} $, encoding the dot product $ a \\cdot x $ in the quantum state.</p> </li> <li><p>Apply Hadamard Gates Again: Applying Hadamard gates again to all but the auxiliary qubit uses quantum interference to amplify the probability amplitudes of the states corresponding to $ a$.</p> </li> <li><p>Measurement: Measure the first $ n $ qubits to directly obtain $a $ in binary form.</p> </li> </ol>"},{"location":"qcfpga/#conclusion-and-significance","title":"Conclusion and Significance\u00b6","text":"<p>The Bernstein-Vazirani algorithm demonstrates quantum parallelism and serves as an introductory example for more complex quantum algorithms like Shor's and Grover's algorithms, highlighting quantum computational speed-ups.</p>"},{"location":"reporting_profiling/","title":"Reporting &amp; Profiling SYCL programs for Intel\u00ae FPGA cards","text":"<p>After having spent some time to write your kernel and debug functional problems, it's now time to take advantage of the accelerator. FPGA uses pipelining parallelism but how does it work ? </p> <p></p>"},{"location":"reporting_profiling/#definitions","title":"Definitions","text":"<p>Pipelining (see FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits)</p> <p>Pipelining is a design technique used in synchronous digital circuits to increase fMAX. Pipelining involves adding registers to the critical path, which decreases the amount of logic between each register. Less logic takes less time to execute, which enables an increase in f MAX. The critical path in a circuit is the path between any two consecutive registers with the highest latency. That is, the path between two consecutive registers where the operations take the longest to complete. Pipelining is especially useful when processing a stream of data. A pipelined circuit can have different stages of the pipeline operating on different input stream data in the same clock cycle, which leads to better data processing throughput. </p> <p>Maximum Frequency (fMAX)</p> <p>The fMAX of a digital circuit is its highest possible clock frequency, determining the maximum rate for updating register outputs. This speed is constrained by the physical propagation delay of the signal across the combinational logic between consecutive register stages. The delay is affected by the complexity of the combinational logic in the path, and the path with the greatest number of logic elements and highest delay sets the speed limit for the entire circuit, often known as the critical path. The fMAX is the reciprocal of this critical path delay, and having a high fMAX is desirable as it leads to better performance when there are no other restrictions. </p> <p>Throughput</p> <p>Throughput in a digital circuit refers to the speed at which data is handled. When there are no other limiting factors, a higher fMAX leads to increased throughput, such as more samples per second. Often synonymous with performance, throughput is frequently used to gauge the effectiveness of a circuit. </p> <p>Latency</p> <p>Latency measures the duration to complete operations in a digital circuit, and it can be assessed for individual tasks or the whole circuit. It can be measured in time units like microseconds or clock cycles, with the latter often preferred. Measuring latency in clock cycles separates it from the circuit's clock frequency, making it easier to understand the real effects of modifications on the circuit's performance. </p> <p>Occupancy</p> <p>The occupancy of a datapath at a specific moment signifies the fraction of the datapath filled with valid data. When looking at a circuit's execution of a program, the occupancy is the mean value from the beginning to the end of the program's run. Parts of the datapath that are unoccupied are commonly called bubbles, akin to a CPU's no-operation (no-ops) instructions, which don't influence the final output. Minimizing these bubbles leads to greater occupancy. If there are no other hindrances, optimizing the occupancy of the datapath will boost throughput.  Occupancy: \\(\\frac{2}{5}=40\\%\\) </p>"},{"location":"reporting_profiling/#reporting","title":"Reporting","text":"<p>If you instruct the DPC++ compiler to stop compiling the design after generating the early image, you will be able to access precious information including performance and area estimates without having to wait many hours. The FPGA Early Image can be analyzed using the FPGA Optimization Report to provide:</p> <ul> <li>loop analysis</li> <li>area estimates</li> <li>kernel memory information </li> <li> <p>scheduler information</p> </li> <li> <p>Recall that the FPGA Early image can be obtained using the command: <code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 accumulator.cpp -o accumulator_report.a</code></p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code> <p>Analyzing the FPGA Early Image report</p> SetupQuestionSolution <ul> <li>First,copy <code>/project/home/p200117/FPGA/05-accumulator</code> to  your home folder    </li> <li>Generate you the early image with the report using:  <pre><code>  # Don't forget to be on a node first\n  icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 accumulator.cpp -o accumulator_report.a`\n</code></pre></li> <li>The next step is to follow the Graphical sessions guide</li> <li>Once connected to the vnc session, you should see something like this:  </li> <li>Open a terminal using the GUI interface and got to <code>05-accumulator/src/accumulator_report.prj/reports/</code> directory</li> <li>Open the file <code>report.html</code> with firefox  </li> </ul> <ul> <li>Check the loop analysis report. What do you observe ?</li> <li>What is the predicted fMAX ?</li> <li>What is the block scheduled II ? What is its impact ? </li> </ul> <ul> <li>We have a data-dependency at line 59</li> <li>For each loop iteration, the Intel\u00ae oneAPI DPC++/C++ Compiler takes 3 cycles to compute the result of the addition and then stores it in the variable temp_sum</li> <li>We either need to remove the data dependency or to relax it</li> </ul> <p>Analyzing the final FPGA Image report</p> QuestionSolution <ul> <li>Open <code>/project/home/p200117/FPGA/06-shift_register/src/shift_register.fpga.prj/reports/report.html</code></li> <li>Did the shift register solve the problem ? What is now the Initialization Interval ?</li> </ul> <ul> <li>We can force the II using <code>#pragma II &lt;N&gt;</code> but it will trigger an error if it can reduced it</li> <li>The shift register is a very efficient design pattern for FPGA programming as it increase the dependence distance between loops.</li> <li>Relaxation means increasing this distance</li> </ul>"},{"location":"reporting_profiling/#profiling","title":"Profiling","text":"<ul> <li>The Intel\u00ae FPGA dynamic profiler for DPC++ can be used to add performance counters to the design and collect performance data during execution. This requires full hardware compilation.</li> <li>In order to instruct the offline compiler to add those performance counters, one needs to add the compilation option <code>-Xsprofile</code>.</li> </ul> Intel\u00ae FPGA Dynamic Profiler for DPC++: Performance Counters Instrumentation <p>Performance counters are attached to every load and store instruction, and they are linked together in a sequence that connects to the Control Register Access (CRA). The CRA interface provides access to the control and status registers interface.</p> <ul> <li>Once the design has been compiled, performance data can be obtained at runtime:<ul> <li>Either using your host application in the Intel\u00ae VTuneTM Profiler with CPU/FPGA Interaction view</li> <li>Or using the command line by using the Profiler Runtime Wrapper. Data can later be importer to the  Intel\u00ae VTuneTM Profiler</li> </ul> </li> </ul>"},{"location":"reporting_profiling/#profiler-runtime-wrapper","title":"Profiler Runtime Wrapper","text":"<ul> <li>You need to execute the FPGA executable using the Profiler Runtime Wrapper to fill the profiling results using the following command: <pre><code>aocl profile [options] /path/to/executable [executable options]\n</code></pre></li> <li>The Profiler Runtime Wrapper calls your executable and collects profile information</li> <li>The performance counter data is saved in a <code>profile.mon</code> monitor description file that the Profiler Runtime Wrapper post-processes and outputs into a readable <code>profile.json</code> file</li> <li> <p>Intel recommends the use of the <code>profile.json</code> for further data processing</p> </li> <li> <p>Note that you can control the sample rate used by the Profiler Runtime Wrapper using the <code>-period &lt;N&gt;</code> option</p> </li> <li> <p>Use the command <code>aocl profile -help</code> to get more details: <pre><code> aocl profile --help\n   aocl profile can be used to collect information about your host run if you compiled with -profile. \n   To use it, please specify your host executable as follows: 'aocl profile path/to/executable'. \n\n   If you are compiling with the oneAPI Data Parallel C++ Compiler and do not wish to pass in the \n   compiled executable binary directly (but rather a script that calls it), the binary needs to be \n   passed in using '--executable' or '-e'. \n\n   It is also optional (but recommended) that you include the location of the *.aocx file \n   using '--aocx' or '-x'. Note that this file is not needed when compiling with the \n   oneAPI Data Parallel C++ Compiler. If no files are given, any aocx found \n   in the current directory will be used (potentially incorrectly) \n\n   OpenCL use case example: aocl profile -x kernel.aocx bin/host -host-arg1 -host-arg2 \n\n   oneAPI use case example: aocl profile -e kernel.fpga executable_calling_kernel_fpga.sh -executable-arg1 \n\n   You can also specify a few other options (after the executable if relevant): \n     - Adjust the period between counter readbacks by specifying the number of clock cycles between subsequent\n       readbacks with '-period ###': 'aocl profile path/to/executable -period 10000' \n       If this flag is not passed in, the counters will be read back as often as possible. \n     - Change counters to only read when the kernel finishes (not while it's running) with -no-temporal \n     - Turn off memory transfer information with -no-mem-transfers \n     - Turn on shared counter data (use when design compiled with '-profile-shared-counters' option) \n     - Change the output directory where the .mon and .json file will be placed with '-output-dir /path/to/new/loc/' \n     - Skip the actual compile and just point to a profile.mon file with '-no-run /path/to/profile.mon' \n       Do this if you already have data, but want it in a format that VTune can display. \n     - Do not create a profile.json file by setting the '-no-json' flag (no need for .aocx or .source files) \n       Do this if you do not wish to visualize the profiler data with VTune, and want the profile.mon output\n\n   Please ensure that the executable and its options are the last arguments. \n</code></pre></p> </li> </ul>"},{"location":"reporting_profiling/#performance-data","title":"Performance data","text":"Stall (%)Occupancy (%)Bandwidth <p>The percentage of time that memory or pipe access leads to pipeline stalls represents a measure of the efficiency of the memory or pipe in fulfilling access requests. It reflects how often these components can successfully meet a request without causing a delay or interruption in the processing flow.</p> <p>the proportion of the total time profiled in which a valid work-item is executing a memory or pipe instruction. It quantifies the fraction of the observed period when actual processing tasks related to memory or pipe instructions are being carried out.</p> <p>The average memory bandwidth refers to the efficiency of memory access in an FPGA. For each global memory access, resources are assigned to obtain data, but the kernel program might use less than the acquired amount. The overall efficiency is the percentage of the total bytes retrieved from the global memory system that the kernel program actually utilizes.</p>"},{"location":"reporting_profiling/#example","title":"Example","text":"<p>Improve Bandwith using vectorization</p> Without SIMDHow to use vectorizationWith SIMD <ul> <li>The folder <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src</code> contains  FPGA image compiled with the option <code>-Xsprofile</code></li> <li>The kernel use a simple data-parallel kernel (no work-groups) to sum two arrays (size=2048) </li> <li>Using the GUI interface used previously, open the VTune software  </li> <li>Create a new project and import <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src/profiling</code> which contains the files <code>profile.json</code> and <code>profile.mon</code> </li> <li>Once loaded, open the \"Bottom-up\" tab </li> <li>We have a high occupancy and a average bandwidth of 4.5 GB/s far from the theoretical bandwidth of 12.8 GB/s for a single pseudo-channel </li> </ul> <ul> <li>You will first need to use a ND-range kernel and define your work-group size using the attribute [[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</li> <li>To specify the number of SIMD work_items, you will need to add the following attribute [[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]] with     NUM_SIMD_WORK_ITEMS dividing evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> <li>Example <pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        \n    [=](sycl::nd_item&lt;1&gt; it) \n    [[intel::num_simd_work_items(8),\n    sycl::reqd_work_group_size(1, 1, 128)]] {\n    auto gid = it.get_global_id(0);\n    accessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n    });\n});\n...\n</code></pre></li> <li>The 128 work-items are evenly distributed among 8 SIMD lanes</li> <li>\\(\\frac{128}{8}\\) = 16 wide vector operation</li> <li>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</li> </ul> <ul> <li>The folder <code>/project/home/p200117/FPGA/08-vector_add_ndrange_profiling_simd/src</code> contains  the vectorized version</li> <li>Load into the current project the <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src/profiling</code> folder which contains the files <code>profile.json</code> and <code>profile.mon</code> </li> <li>The bandwidth is now 9.6 GB/s</li> </ul>"},{"location":"tmux_screen/","title":"Persistent terminal sessions","text":""},{"location":"tmux_screen/#persistent-terminal-sessions-using-gnu-screen","title":"Persistent Terminal Sessions using GNU Screen","text":"<p>GNU Screen is a tool to manage persistent terminal sessions. It becomes interesting since you will probably end at some moment with the following  scenario:</p> <p>you frequently program and run computations on the UL HPC platform i.e on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.</p> <p>Probably what you do in the above scenario is to</p> <p>a. clear and shutdown all running terminal sessions</p> <p>b. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )</p> <p>c. repeat the following morning when you come back to the office.</p> <p>Enter the long-existing and very simple, but totally indispensable GNU screen command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.</p>"},{"location":"tmux_screen/#screen-commands","title":"Screen commands","text":"<p>You can start a screen session (i.e. creates a single window with a shell in it) with the <code>screen</code> command. Its main command-lines options are listed below:</p> <ul> <li><code>screen</code>: start a new screen</li> <li><code>screen -ls</code>: does not start screen, but prints a list of <code>pid.tty.host</code> strings identifying your current screen sessions.</li> <li><code>screen -r</code>: resumes a detached screen session</li> <li><code>screen -x</code>: attach to a not detached screen session. (Multi display mode i.e. when you and another user are trying to access the same session at the same time)</li> </ul> <p>Once within a screen, you can invoke a screen command which consist of a \"<code>CTRL + a</code>\" sequence followed by one other character. The main commands are:</p> <ul> <li><code>CTRL + a c</code>: (create) creates a new Screen window. The default Screen number is zero.</li> <li><code>CTRL + a n</code>: (next) switches to the next window.</li> <li><code>CTRL + a p</code>: (prev) switches to the previous window.</li> <li><code>CTRL + a d</code>: (detach) detaches from a Screen</li> <li><code>CTRL + a A</code>: (title) rename the current window</li> <li><code>CTRL + a 0-9</code>: switches between windows 0 through 9.</li> <li><code>CTRL + a k</code> or <code>CTRL + d</code>: (kill) destroy the current window</li> <li><code>CTRL + a ?</code>: (help) display a list of all the command options available for Screen.</li> </ul>"},{"location":"tmux_screen/#persistent-terminal-sessions-using-tmux","title":"Persistent Terminal Sessions using Tmux","text":"<p>Tmux is a more modern equivalent to GNU screen.</p>"},{"location":"tmux_screen/#tmux-commands","title":"Tmux commands","text":"<p>You can start a tmux session (i.e. creates a single window with a shell in it) with the <code>tmux</code> command. Its main command-lines options are listed below:</p> <ul> <li><code>tmux</code>: start a new tmux session</li> <li><code>tmux ls</code>: does not start tmux, but print the list of the existing sessions.</li> <li><code>tmux a</code>: resumes a detached tmux session</li> </ul> <p>Once within a tmux, you can invoke a tmux command which consist of a \"<code>CTRL + b</code>\" sequence followed by one other character. The main commands are:</p> <ul> <li><code>CTRL + b c</code>: (create) creates a new tmux window. The default tmux number is zero.</li> <li><code>CTRL + b n</code>: (next) switches to the next window.</li> <li><code>CTRL + b p</code>: (prev) switches to the previous window.</li> <li><code>CTRL + b d</code>: (detach) detaches from a session</li> <li><code>CTRL + b ,</code>: (title) rename the current window</li> <li><code>CTRL + b 0-9</code>: switches between windows 0 through 9.</li> <li><code>CTRL + d</code>: (kill) destroy the current window</li> <li><code>CTRL + b ?</code>: (help) display a list of all the command options available for tmux.</li> </ul>"},{"location":"writing/","title":"Developing SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"writing/#anatomy-of-a-sycl-program","title":"Anatomy of a SYCL program","text":""},{"location":"writing/#data-management","title":"Data Management","text":"<p>In the context of SYCL, Unified Shared Memory (USM) and buffers represent two different ways to handle memory and data management. They offer different levels of abstraction and ease of use, and the choice between them may depend on the specific needs of an application. Here's a breakdown of the differences:</p>"},{"location":"writing/#unified-shared-memory-usm","title":"Unified Shared Memory (USM)","text":"<p>Unified Shared Memory is a feature that simplifies memory management by providing a shared memory space across the host and various devices, like CPUs, GPUs, and FPGAs. USM provides three different types of allocations:</p> <ol> <li>Device Allocations: Allocated memory is accessible only by the device.</li> <li>Host Allocations: Allocated memory is accessible by the host and can be accessed by devices. However, the allocated memory is stored on the host global memory. </li> <li>Shared Allocations: Allocated memory is accessible by both the host and devices. The allocated memory is present in both global memories and it is synchronized between host and device.</li> </ol> <p>USM allows for more straightforward coding, akin to standard C++ memory management, and may lead to code that is easier to write and maintain. </p> <p>FPGA support</p> <p>SYCL USM host allocations are only supported by some BSPs, such as the Intel\u00ae FPGA Programmable Acceleration Card (PAC) D5005 (previously known as Intel\u00ae FPGA Programmable Acceleration Card (PAC) with Intel\u00ae Stratix\u00ae 10 SX FPGA). Check with your BSP vendor to see if they support SYCL USM host allocations.</p> <p>Using SYCL, you can verify if you have access to the different features:</p> <p>Verify USM capabilities</p> <pre><code>if (!device.has(sycl::aspect::usm_shared_allocations)) {\n    # Try to default to host allocation only\n    if (!device.has(sycl::aspect::usm_host_allocations)) {\n        # Default to device and explicit data movement\n        std::array&lt;int,N&gt; host_array;\n        int *my_array = malloc_device&lt;int&gt;(N, Q);\n    }else{\n        # Ok my_array is located on host memory but transferred to device as needed\n        int* my_array = malloc_host&lt;int&gt;(N, Q);\n    }\n}else{\n        # Ok my_array is located on both global memories and synchronized automatically \n        int* shared_array = malloc_shared&lt;int&gt;(N, Q);\n}\n</code></pre> <p>That's not all</p> <ul> <li>Concurrent accesses and atomic modificationes are not necessarily available even if you have host and shared capabilities.</li> <li>You need to verify <code>aspect::usm_atomic_shared_allocations</code> and <code>aspect::usm_atomic_host_allocations</code>.</li> </ul> <p>Bittware 520N-MX</p> <p>The USM host allocations is not supported by some BSPs. We will therefore use explicit data movement</p> <p>Explicit USM</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Replace the original code with explicit USM code </li> <li>Verify your code using emulation</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int host_vec_a[kVectSize];\n    int host_vec_b[kVectSize];\n    int host_vec_c[kVectSize];\n    int * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    q.memcpy(vec_a, host_vec_a, kVectSize * sizeof(int)).wait();\n    q.memcpy(vec_b, host_vec_b, kVectSize * sizeof(int)).wait();\n\n\n\n    q.single_task&lt;VectorAddID&gt;([=]() {\n        VectorAdd(vec_a, vec_b, vec_c, kVectSize);\n      }).wait();\n\n    q.memcpy(host_vec_c, vec_c, kVectSize * sizeof(int)).wait();\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = host_vec_a[i] + host_vec_b[i];\n      if (host_vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; host_vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; host_vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; host_vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    sycl::free(vec_a,q);\n    sycl::free(vec_b,q);\n    sycl::free(vec_c,q);\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#buffer-accessors","title":"Buffer &amp; accessors","text":"<p>Buffers and accessors are key abstractions that enable memory management and data access across various types of devices like CPUs, GPUs, DSPs, etc.</p> <ol> <li> <p>Buffers:Buffers in SYCL are objects that represent a region of memory accessible by the runtime. They act as containers for data and provide a way to abstract the memory management across host and device memories. This allows for efficient data movement and optimization by the runtime, as it can manage the data movement between host and device memory transparently.</p> </li> <li> <p>Accessors:Accessors provide a way to access the data inside buffers. They define the type of access (read, write, read-write) and can be used within kernels to read from or write to buffers.</p> </li> </ol> <p>Advantage</p> <p>Through the utilization of these accessors, the SYCL runtime examines the interactions with the buffers and constructs a dependency graph that maps the relationship between host and device functions. This enables the runtime to automatically orchestrate the transfer of data and the sequencing of kernel activities.</p> <p>Using Buffers and Accessors</p> <pre><code>    #include &lt;array&gt; \n    // oneAPI headers\n    #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n    #include &lt;sycl/sycl.hpp&gt;\n\n    class Kernel;\n    constexpr int N = 100;\n    std::array&lt;int,N&gt; in_array;\n    std::array&lt;int,N&gt; out_array;\n    for (int i = 0 ; i &lt;N; i++)\n        in_array[i] = i+1;\n    queue device_queue(sycl::ext::intel::fpga_selector_v);\n\n    { // This one is very important to define the buffer scope\n      // buffer&lt;int, 1&gt; in_device_buf(in.data(), in.size());\n      // Or more convenient\n\n      buffer in_device_buf(in_array);\n      buffer out_device_buf(out_array);\n      device_queue.submit([&amp;](handler &amp;h) {\n        accessor in(in_device_buf, h, read_only);\n        accessor out(out_device_buf, h, write_only, no_init);\n        h.single_task&lt;Kernel&gt;([=]() { });\n      };\n    } \n    // Accessor going out of the scope\n    // Data has been copied back !!!\n</code></pre> <p>What about memory accesses in FPGA ? </p> <p>For FPGAs, the access pattern, access width, and coalescing of memory accesses can significantly affect performance. You might want to make use of various attributes and pragmas specific to your compiler and FPGA to guide the compiler in optimizing memory accesses.</p> <ul> <li>The <code>vector_add.cpp</code> source code introduced in the compiling section relies on buffers and accessors. Although DPC++ is built on top of SYCL, the use of specific hardware needs some attentions</li> </ul> <p>Executing the FPGA bitstream</p> QuestionSolution <ul> <li>Go to <code>/project/home/p200117/FPGA</code></li> <li>We have build to different FPGA bitstream versions of <code>vector_add.cpp</code>:</li> <li>Go to the folder <code>01-no_data_alignment/src</code> and execute the code on the FPGA card. What do you see ?</li> <li>Now go to the folder <code>02-with_data_alignment/src</code> and execute the code on the FPGA card. How could we align data properly ?</li> </ul> <p><pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\nPASSED\n</code></pre> Replace the following lines: <pre><code>    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n</code></pre> by these ones: <pre><code>   int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize]; \n</code></pre></p>"},{"location":"writing/#queue","title":"Queue","text":"<p>Contrary to OpenCL, queues in SYCL are out-of-order by default. Nonetheless, you can change this behavior you declare it in your code.</p> <p>In-order-queue</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v,{property::queue::in_order()}};\n  // Task A\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  // Task B\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  }); \n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nB[TaskB] --&gt; C[TaskC];</code></pre> </p> <p>This behavior is not very useful nor flexible. Queue objects, by default, are out-of-order queues, except when they're constructed with the in-order queue property. Because of this, they must include mechanisms to arrange tasks that are sent to them. The way queues organize tasks is by allowing the user to notify the runtime about the dependencies that exist between these tasks. These dependencies can be described in two ways: either explicitly or implicitly, through the use of command groups.</p> <p>A command group is a specific object that outlines a task and its dependencies. These groups are generally expressed as C++ lambdas and are handed over as arguments to the submit() method within a queue object. The single parameter within this lambda is a reference to a handler object, utilized inside the command group to define actions, generate accessors, and outline dependencies.</p>"},{"location":"writing/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Like for OpenCL, you can manage dependencies explicitly using events. </p> <p>Using events</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v};\n  // Task A\n  auto event_A = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  event_A.wait();\n  // Task B\n  auto event_B = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  auto event_C = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  });\n  // Task D\n  device_queue.submit([&amp;](handler &amp;h) {\n  h.depends_on({event_B, event_C});\n  h.parallel_for(N, [=](id&lt;1&gt; i) { /*...*/ });\n  }).wait();\n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nA[TaskA] --&gt; C[TaskC];\nB[TaskB] --&gt; D[TaskD];\nC[TaskC] --&gt; D[TaskD];</code></pre> </p> <ul> <li>Explicit dependencies using events is relevant when you use USM since buffers make use of accessors to model data dependencies.</li> <li>They are three possibilities to declare a dependcies explicitely:</li> <li>Calling the method <code>wait()</code> on the queue it-self</li> <li>Calling the method <code>wait</code> on the event return by the queue after submitting a command</li> <li>Calling the method <code>depends_on</code> of the handler object</li> </ul>"},{"location":"writing/#implicit-dependencies","title":"Implicit dependencies","text":"<ul> <li>Implicit dependencies occurs when your are using buffer &amp; accessor.</li> <li> <p>Accessors have different access modes:</p> </li> <li> <p>read_only: The content of the buffer can only be accessed for reading. So the content will only be copied once to the device</p> </li> <li>write_only: The content of the buffer can only be accessed for writing. The content of buffer is still copied from host to device before the kernel starts </li> <li>read_write: The content of the buffer can be accessed for reading and writing.</li> </ul> <p>You can add the <code>no_init</code> property to an accessor in <code>write_only</code> mode. This tells the runtime that the original data contains in the buffer can be ignored and don't need to be copied from host to device.</p> <p>Implicit dependencies obey to three main patterns (see DPC++ book):</p> <ul> <li>Read-after-Write  (RAW) : occurs when some data modified by a kernel should be read by another kernel. </li> <li>Write-after-Read  (WAR) : occurs when some data read by a kernel will be modified by another one</li> <li>Write-after-Write (WAW) : occurs when two kernels modified the same data</li> </ul> <p>Implicit dependencies</p> QuestionSolution <ul> <li>By default without access mode, each accessor will be read_write inducing unnecessary copies.</li> <li>Note also the first use of <code>host_accessor</code>. Why did we use it here ?</li> <li>Modifiy the following code to take into account implicit dependencies.  <pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h};\n      accessor aB{B, h};\n      accessor aC{C, h};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h};\n       accessor aB{B, h};\n       accessor aC{C, h};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C};\n</code></pre></li> </ul> <pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h, write_only, no_init};\n      accessor aB{B, h, write_only, no_init};\n      accessor aC{C, h, write_only, no_init};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h, read_only};\n       accessor aB{B, h, read_only};\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C, read_only};\n</code></pre>"},{"location":"writing/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<ul> <li>FPGA strongly differs from ISA-based hardware such as CPU and GPU</li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize the everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p></p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Pipelining example provided chap.17 (DPC++ book)</p> Processing a single element (Figure. 17-13)Taking advantage of pipelining (Figure 17-14) <p></p> <ul> <li>The pipeline is mostly empty.</li> <li>Hardware units are not busy and the efficiency is thus low.</li> </ul> <p></p> <ul> <li>More data than stages, the pipeline is full and all hardware units are busy.</li> </ul> <p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"writing/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of thread started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16  <ul> <li>In order to write basic data-parallel kernel, you will need to use the <code>parallel_for</code> method. Below is an example of simple data-parallel kernel. As you can notice it, there is no notion of groups nor sub-groups. </li> </ul> <p>Matrix addition</p> <pre><code>   constexpr int N = 2048;\n   constexpr int M = 1024;\n   queue.submit([&amp;](sycl::handler &amp;h) {\n     sycl::accessor acc_a{buffer_a, h, sycl::read_only};\n     sycl::accessor acc_b{buffer_b, h, sycl::read_only};\n     sycl::accessor acc_c{buffer_c, h, sycl::read_write, sycl::no_init};\n     h.parallel_for(range{N, M}, [=](sycl::id&lt;2&gt; idx) {\n      acc_c[idx] = acc_a[idx] + acc_b[idx];\n     });\n   });\n</code></pre> <p>Vector addition</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Adapt the <code>vector_add.cpp</code> single-task kernel to a basis data-parallel kernel</li> <li>Emulate to verify your design</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nconstexpr int kVectSize = 256;\n\nint main() {\nbool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::write_only, sycl::no_init};\n\n        h.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\n      accessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>If you want to have a fine-grained control of your data-parallel kernel, ND-range data-parallel kernels are the equivalent of ND-range kernels in OpenCL. </li> </ul> <p>ND-range kernel in SYCL</p> <ul> <li><code>nd_range(range&lt;dimensions&gt; globalSize, range&lt;dimensions&gt; localSize);</code></li> <li>ND-range kernels are defined with two range objects<ul> <li>global representing the total size of work-items</li> <li>local representing the size of work-groups</li> </ul> </li> </ul> <p>Tiled Matrix Multiplication</p> QuestionSolution <ul> <li>Fill the blank and complete the code  <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    constexpr size_t N = 512;\n    constexpr size_t B =  16;\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      /* DEFINE HERE the global size and local size ranges*/\n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n        h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            // Provide local indexes i and j -- fill here\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              // Do the same for tileB\n              // fill here \n              item.barrier();\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n       sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n         // Fill here \n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n  // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++){\n         float true_val=0.0;\n         for (int k = 0 ; k &lt; N; k++){\n           true_val += mat_a[i*N +k] * mat_b[k*N+j];\n         }\n         if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n            std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n            passed = false;\n         }\n    }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    constexpr size_t N = 512;\n    constexpr size_t B =  16;\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      sycl::range global {N,N};\n      sycl::range local  {B,B}; \n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n       h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            int i = item.get_local_id()[0];\n           int j = item.get_local_id()[1];\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              tileB[i][j] = accessor_b[p*B+i][n];\n              item.barrier();\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n               sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n              // reads from the local memory tile are complete.\n              item.barrier();\n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n   for (int j = 0; j &lt; N; j++){\n      float true_val=0.0;\n      for (int k = 0 ; k &lt; N; k++){\n       true_val += mat_a[i*N +k] * mat_b[k*N+j];\n      }\n          if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n               std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n               passed = false;\n           }\n   }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Warning on work-items group size</p> <ul> <li>If the attribute [[intel::max_work_group_size(Z, Y, X)]] is not specified in your kernel, the workgroup size assumes a default value depending on compilation time and runtime constraints</li> <li>If your kernel contains a barrier, the Intel\u00ae oneAPI DPC++/C++ Compiler sets a default maximum scalarized work-group size of 128 work-items ==&gt; without this attribute, the previous ND-Range kernel would have failed since we have a local work-group size of B x B = 256 work-items </li> </ul>"},{"location":"writing/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the programmer need not do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul> <p>Single-work item creation</p> <ul> <li>Replace the <code>parallel_for</code>method by the <code>single_task</code> method defined in the handler class to create a single-work item kernel</li> <li>The source file <code>vector_add.cpp</code> from <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code> uses loop pipelining.</li> </ul> <pre><code>  #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n  #include &lt;sycl/sycl.hpp&gt;\n\n  using namespace sycl;\n\n  int main(){\n\n\n  // queue creation &amp; data initialization\n\n\n   q.submit([&amp;](handler &amp;h) {\n     h.single_task&lt;class MyKernel&gt;([=]() {\n       // Code to be executed as a single task\n     });\n   });\n   q.wait();\n  }\n</code></pre> <p>Inferring a shift register -- the accumulator case</p> ProblemQuestionSolution <ul> <li>The following code sums double precision floating-point array</li> <li>The problem is the following one:</li> <li>For each loop iteration, the Intel\u00ae oneAPI DPC++/C++ Compiler takes &gt;1 cycles to compute the result of the addition and then stores it in the variable temp_sum</li> <li>So you have a data dependency on temp_sum  <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    double * vec = new(std::align_val_t{ 64 }) double[kVectSize];\n    double res = 0;\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec[i] = 1.0;\n    }\n\n    std::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_in{vec, sycl::range(kVectSize)};\n      sycl::buffer buffer_out{&amp;res, sycl::range(1)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor arr{buffer_in, h, sycl::read_only};\n        sycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\n\n        h.single_task&lt;Accumulator&gt;([=]() {\n     double temp_sum = 0;\n          for (int i = 0; i &lt; kVectSize; ++i)\n            temp_sum += arr[i];\n          result[0] = temp_sum;\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Accumulation is correct\n    double expected = 0.0; \n    for (int i = 0; i &lt; kVectSize; i++) \n      expected += vec[i];\n\n    if (res != expected) {\n        std::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n                  &lt;&lt; expected &lt;&lt; std::endl;\n        passed = false;\n      }\n\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <ul> <li>The following code rely on a shift register to relax the data dependency</li> <li>Fill in the blank to complete the implementation  <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\n\nconstexpr int kVectSize = 256;\n// Initialization cycle (let us take a bit more than 10)\nconstexpr int II_CYCLES = 12;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    double * vec = new(std::align_val_t{ 64 }) double[kVectSize];\n    double res = 0;\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec[i] = 1.0;\n    }\n\n    std::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_in{vec, sycl::range(kVectSize)};\n      sycl::buffer buffer_out{&amp;res, sycl::range(1)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor arr{buffer_in, h, sycl::read_only};\n        sycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\n\n        h.single_task&lt;Accumulator&gt;([=]() {\n       //Create shift register with II_CYCLE+1 elements\n       double shift_reg[II_CYCLES+1];\n       //Initialize all elements of the register to 0\n       //You must initialize the shift register \n        // fill here\n\n       //Iterate through every element of input array\n       for(int i = 0; i &lt; kVectSize; ++i){\n          //Load ith element into end of shift register\n          //if N &gt; II_CYCLE, add to shift_reg[0] to preserve values\n          shift_reg[II_CYCLES] = shift_reg[0] + arr[i];\n\n          #pragma unroll\n          //Shift every element of shift register\n          //Done in 1 cycle if using loop unrolling\n           // fill here\n\n        }\n       //Sum every element of shift register\n       double temp_sum = 0;\n       #pragma unroll\n       for(int i = 0; i &lt; II_CYCLES; ++i){\n             temp_sum += shift_reg[i];\n        }\n        result[0] = temp_sum;\n           });\n         });\n       }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Accumulation is correct\n    double expected = 0.0; \n    for (int i = 0; i &lt; kVectSize; i++) \n      expected += vec[i];\n\n    if (res != expected) {\n        std::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n                  &lt;&lt; expected &lt;&lt; std::endl;\n        passed = false;\n      }\n\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\n\nconstexpr int kVectSize = 256;\n// Initialization cycle (let us take a bit more than 10)\nconstexpr int II_CYCLES = 12;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    double * vec = new(std::align_val_t{ 64 }) double[kVectSize];\n    double res = 0;\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec[i] = 1.0;\n    }\n\n    std::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_in{vec, sycl::range(kVectSize)};\n      sycl::buffer buffer_out{&amp;res, sycl::range(1)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor arr{buffer_in, h, sycl::read_only};\n        sycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\n\n        h.single_task&lt;Accumulator&gt;([=]() {\n        //Create shift register with II_CYCLE+1 elements\n        double shift_reg[II_CYCLES+1];\n        //Initialize all elements of the register to 0\n        //You must initialize the shift register \n        for (int i = 0; i &lt; II_CYCLES + 1; i++) {\n          shift_reg[i] = 0;\n        }\n        //Iterate through every element of input array\n        for(int i = 0; i &lt; kVectSize; ++i){\n           //Load ith element into end of shift register\n           //if N &gt; II_CYCLE, add to shift_reg[0] to preserve values\n           shift_reg[II_CYCLES] = shift_reg[0] + arr[i];\n\n           #pragma unroll\n           //Shift every element of shift register\n           //Done in 1 cycle if using loop unrolling\n           for(int j = 0; j &lt; II_CYCLES; ++j){\n              shift_reg[j] = shift_reg[j + 1];\n           }\n        } \n       //Sum every element of shift register\n       double temp_sum = 0;\n       #pragma unroll\n       for(int i = 0; i &lt; II_CYCLES; ++i){\n          temp_sum += shift_reg[i];\n       }\n       result[0] = temp_sum;\n             });\n           });\n         }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Accumulation is correct\n    double expected = 0.0; \n    for (int i = 0; i &lt; kVectSize; i++) \n      expected += vec[i];\n\n    if (res != expected) {\n        std::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n                  &lt;&lt; expected &lt;&lt; std::endl;\n        passed = false;\n      }\n\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#summary","title":"Summary","text":"<p>We have seen</p> <ul> <li>The anatomy of SYCL program</li> <li>How to manage data movement between host and device for FPGA<ul> <li>Explicit data movement with USM</li> <li>Implicit data movement with Buffers &amp; accessors  </li> </ul> </li> <li>How to manage data dependencies between kernels<ul> <li>Explicit dependencies with events</li> <li>Implicit dependencies using buffers access mode </li> </ul> </li> <li>How to define kernels and the importance of pipelining in FPGA<ul> <li>ND-range kernel created with the <code>parallel_for</code> method</li> <li>Single-work item kernel with the <code>single_task</code> method</li> </ul> </li> </ul> <p>We did not see</p> <ul> <li>Hierachical Parallels kernels</li> <li>Memory models and atomics</li> <li>The DPC++ Parallel STL</li> </ul>"}]}