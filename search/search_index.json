{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Quantum Exact Simulation with Intel\u00ae FPGA","text":"<p>Quantum computing exact simulations involve the use of classical computers to model and analyze the behavior of quantum systems and quantum algorithms, bridging the gap between theoretical quantum mechanics and practical quantum computing technologies. These simulations are crucial for developing, testing, and optimizing quantum algorithms before they are run on actual quantum hardware, which is often limited in availability and capability.</p>"},{"location":"#existing-simulators","title":"Existing simulators","text":"<p>There are several quantum simulators available that vary in their approach, capabilities, and the scale of systems they can simulate. Among the most famous one, you have:</p> <ul> <li> <p>Qiskit Aer: Developed by IBM, Qiskit Aer is an open-source simulator that allows users to perform realistic simulations of quantum circuits, complete with noise models and resource estimations. It helps in understanding how quantum algorithms will perform on real quantum hardware.</p> </li> <li> <p>Cirq: Google's Cirq is another open-source framework designed to simulate and test quantum algorithms on local machines. It is particularly tailored for noisy intermediate-scale quantum (NISQ) computers.</p> </li> <li> <p>cuQuantum: NVIDIA's specialized SDK (Software Development Kit) for accelerating quantum computing simulations on GPUs. Announced and developed by NVIDIA, this toolkit is designed to harness the parallel processing capabilities of GPUs to speed up the simulation of quantum circuits and quantum systems. cuQuantum targets both researchers and developers in the field of quantum computing, providing tools and libraries optimized for NVIDIA's GPU architecture.</p> </li> </ul> <p>Simulations play a dual role in the quantum computing landscape. They are instrumental in:</p> <ul> <li> <p>Algorithm Development: By simulating the outcomes of quantum algorithms, researchers can identify potential improvements and optimizations without needing access to quantum processors.</p> </li> <li> <p>Hardware Design: Simulations help predict how quantum devices might perform in the real world, aiding in the design and construction of more effective quantum hardware.</p> </li> </ul>"},{"location":"#hardware-accelerators-ha","title":"Hardware accelerators (HA)","text":"<p>Hardware accelerators provide significant advantages for simulating quantum systems due to their powerful parallel processing capabilities. In the field of quantum computing, where simulating quantum phenomena on classical computers can be computationally intensive, HA help in addressing some of these challenges by accelerating calculations.</p> <ul> <li> <p>Parallelism: Quantum simulations involve operations on large vectors and matrices since the state of a quantum system is represented by a state vector in a complex vector space, and operations on these states are represented by matrices. For example, GPUs are well-suited for these tasks due to their highly parallel architecture, allowing for faster processing of these large-scale linear algebra operations compared to traditional CPUs.</p> </li> <li> <p>Scalability: While the exponential growth of the quantum state space with each added qubit remains a challenge, HA help push the boundaries of what size systems can be simulated. They enable researchers to simulate slightly larger quantum systems than would be feasible with CPUs alone.</p> </li> <li> <p>Efficiency: For specific types of quantum simulations, such as those involving tensor networks or state vector simulations, HA can significantly speed up the computation. This efficiency is crucial for exploring more complex quantum algorithms and systems within practical time frames.</p> </li> <li> <p>Although less known than GPUs, FPGAs (Field-Programmable Gate Arrays) represent a growing area of interest in the field of quantum computing due to FPGAs' unique properties.</p> </li> <li> <p>FPGAs are integrated circuits that can be configured by the user after manufacturing, allowing for highly specialized hardware setups tailored to specific computational tasks.</p> </li> </ul> <p></p>"},{"location":"#why-using-fpgas-for-quantum-simulations","title":"Why using FPGAs for Quantum Simulations?","text":"<ul> <li> <p>Customizability and Reconfigurability: Unlike CPUs and GPUs, which have fixed architectures, FPGAs can be programmed to create custom hardware configurations. This allows for the optimization of specific algorithms or processes, which can be particularly beneficial for quantum simulations, where different algorithms might benefit from different hardware optimizations.</p> </li> <li> <p>Parallel Processing: FPGAs can be designed to handle parallel computations natively using pipeline parallelism.</p> </li> <li> <p>Low Latency and High Throughput: FPGAs can provide lower latency than CPUs and GPUs because they can be programmed to execute tasks without the overhead of an operating system or other software layers. This makes them ideal for real-time processing and simulations.</p> </li> <li> <p>Energy Efficiency: FPGAs can be more energy-efficient than GPUs and CPUs for certain tasks because they can be stripped down to only the necessary components required for a specific computation, reducing power consumption.</p> </li> </ul>"},{"location":"#intel-fpga-sdk-oneapi-for-fpga","title":"Intel\u00ae FPGA SDK &amp; oneAPI for FPGA","text":"<ul> <li>The Intel\u00ae FPGA Software Development Kit (SDK) provides a comprehensive set of development tools and libraries specifically designed to facilitate the design, creation, testing, and deployment of applications on Intel's FPGA hardware. The SDK includes tools for both high-level and low-level programming, including support for hardware description languages like VHDL and Verilog, as well as higher-level abstractions using OpenCL or HLS (High-Level Synthesis). This makes it easier for developers to leverage the power of FPGAs without needing deep expertise in hardware design.</li> </ul> <ul> <li>Intel\u00ae oneAPI is a unified programming model designed to simplify development across diverse computing architectures\u2014CPUs, GPUs, FPGAs, and other accelerators. The oneAPI for FPGA component specifically targets the optimization and utilization of Intel FPGAs. It allows developers to use a single, consistent programming model to target various hardware platforms, facilitating easier code reuse and system integration. oneAPI includes specialized libraries and tools that enable developers to maximize the performance of their applications on Intel FPGAs while maintaining a high level of productivity and portability.</li> </ul> <p>In this course, you will learn to:</p> <ul> <li> <p>How to use Meluxina's FPGA, i.e., Intel\u00ae FPGA</p> </li> <li> <p>How to exploit FPGA for quantum simulation</p> </li> <li> <p>How to take advantage of the Intel\u00ae oneAPI to code quantum circuit</p> </li> </ul> <p>Remark</p> <p>This course is not intended to be exhaustive. In addition, the described tools and features are constantly evolving. We try our best to keep it up to date. </p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<ul> <li> <p>This course is for students, researchers, enginners wishing to discover how to use oneAPI to program FPGA in this fantastic fields which is Quantum Computing. Participants should still have some experience with Python &amp; modern C++ (e.g., Lambdas, class deduction templates).</p> </li> <li> <p>This course is NOT a Quantum Computing course but intends to show you how to use QC simulation on Meluxina's FPGA.</p> </li> <li> <p>We strongly recommend to interested particpants this CERN online course.</p> </li> </ul>"},{"location":"#about-this-course","title":"About this course","text":"<p>This course has been developed by the Supercomputing Application Services group at LuxProvide. </p>"},{"location":"compile/","title":"Compiling SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"compile/#setups","title":"Setups","text":"<p>Please clone first the oneAPI-sample repository with the <code>git clone --depth 1 https://github.com/oneapi-src/oneAPI-samples.git</code> in your home folder.</p> <p>Once the repository cloned, you should see the following hierarchy:</p> <pre><code>tree -d -L 2 oneAPI-samples\noneAPI-samples\n\u251c\u2500\u2500 AI-and-Analytics\n\u2502   \u251c\u2500\u2500 End-to-end-Workloads\n\u2502   \u251c\u2500\u2500 Features-and-Functionality\n\u2502   \u251c\u2500\u2500 Getting-Started-Samples\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 Jupyter\n\u251c\u2500\u2500 common\n\u2502   \u2514\u2500\u2500 stb\n\u251c\u2500\u2500 DirectProgramming\n\u2502   \u251c\u2500\u2500 C++\n\u2502   \u251c\u2500\u2500 C++SYCL\n\u2502   \u251c\u2500\u2500 C++SYCL_FPGA\n\u2502   \u2514\u2500\u2500 Fortran\n\u251c\u2500\u2500 Libraries\n\u2502   \u251c\u2500\u2500 oneCCL\n\u2502   \u251c\u2500\u2500 oneDAL\n\u2502   \u251c\u2500\u2500 oneDNN\n\u2502   \u251c\u2500\u2500 oneDPL\n\u2502   \u251c\u2500\u2500 oneMKL\n\u2502   \u2514\u2500\u2500 oneTBB\n\u251c\u2500\u2500 Publications\n\u2502   \u251c\u2500\u2500 DPC++\n\u2502   \u2514\u2500\u2500 GPU-Opt-Guide\n\u251c\u2500\u2500 RenderingToolkit\n\u2502   \u251c\u2500\u2500 GettingStarted\n\u2502   \u2514\u2500\u2500 Tutorial\n\u251c\u2500\u2500 Templates\n\u2502   \u2514\u2500\u2500 cmake\n\u2514\u2500\u2500 Tools\n    \u251c\u2500\u2500 Advisor\n    \u251c\u2500\u2500 ApplicationDebugger\n    \u251c\u2500\u2500 Benchmarks\n    \u251c\u2500\u2500 GPU-Occupancy-Calculator\n    \u251c\u2500\u2500 Migration\n    \u2514\u2500\u2500 VTuneProfiler\n</code></pre> <ul> <li>As you can see Intel provides numerous code samples and examples to help your grasping the power of the oneAPI toolkit.</li> <li>We are going to focus on <code>DirectProgramming/C++SYCL_FPGA</code>.</li> <li> <p>Create a symbolic at the root of your home directory pointing to this folder: <pre><code>ln -s oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/GettingStarted\ntree -d -L 2 GettingStarted\nGettingStarted\n\u251c\u2500\u2500 fast_recompile\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 fpga_compile\n\u2502   \u251c\u2500\u2500 part1_cpp\n\u2502   \u251c\u2500\u2500 part2_dpcpp_functor_usm\n\u2502   \u251c\u2500\u2500 part3_dpcpp_lambda_usm\n\u2502   \u2514\u2500\u2500 part4_dpcpp_lambda_buffers\n\u2514\u2500\u2500 fpga_template\n    \u2514\u2500\u2500 src\n</code></pre></p> </li> <li> <p>The fpga_compile folder provides basic examples to start compiling SYCL C++ code with the DPC++ compiler</p> </li> <li> <p>The fpga_recompile folder show you how to recompile quickly your code without having to rebuild the FPGA image</p> </li> <li> <p>The fpga_template is a starting template project that you can use to bootstrap a project</p> </li> </ul>"},{"location":"compile/#discovering-devices","title":"Discovering devices","text":"<p>Before targeting a specific hardware accelerator, you need to ensure that the sycl runtime is able to detect it.</p> <p>Commands</p> <pre><code># Create permanent tmux session\ntmux new -s fpga_session\n# We need a job allocation on a FPGA node\nsalloc -A &lt;account&gt; -t 48:00:00 -q default -p fpga -N 1\n# Load the staging environment\nmodule load env/staging/2023.1\nmodule load intel-fpga\nmodule load 520nmx/20.4\n# Check the available devices\nsycl-ls\n</code></pre> <p>Output</p> <pre><code>[opencl:cpu:0] Intel(R) OpenCL, AMD EPYC 7452 32-Core Processor                 3.0 [2022.13.3.0.16_160000]\n[opencl:acc:1] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2022.13.3.0.16_160000]\n[opencl:acc:2] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0) 1.0 [2022.1]\n[opencl:acc:3] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie1) 1.0 [2022.1]\n</code></pre>"},{"location":"compile/#first-code","title":"First code","text":"<p>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src/vector_add.cpp</p> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\nint len) {\nfor (int idx = 0; idx &lt; len; idx++) {\nint a_val = vec_a_in[idx];\nint b_val = vec_b_in[idx];\nint sum = a_val + b_val;\nvec_c_out[idx] = sum;\n}\n}\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint * vec_a = new int[kVectSize];\nint * vec_b = new int[kVectSize];\nint * vec_c = new int[kVectSize];\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec_a[i] = i;\nvec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\nsycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\nsycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nh.single_task&lt;VectorAddID&gt;([=]() {\nVectorAdd(&amp;accessor_a[0], &amp;accessor_b[0], &amp;accessor_c[0], kVectSize);\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = vec_a[i] + vec_b[i];\nif (vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec_a;\ndelete[] vec_b;\ndelete[] vec_c;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>The <code>vector_add.cpp</code> source file contains all the necessary to understand how to create a SYCL program</p> </li> <li> <p>lines 4 and 5 are the minimal headers to include in your SYCL program</p> </li> <li> <p>line 9 is a forward declaration of the kernel name</p> </li> <li> <p>lines 11-19 is a function representing our kernel. Note the absence of <code>__kernel</code>, <code>__global</code> as it exists in OpenCL</p> </li> <li> <p>lines 30-36 are pragmas defining whether you want a full compilation, a CPU emulation or the simulator</p> </li> <li> <p>line 39 is the queue creation. The queue is bounded to a device. We will discuss it later in details.</p> </li> <li> <p>lines 41-46 provides debugging information at runtime.</p> </li> <li> <p>lines 48-54 instantiates 3 vectors. <code>vec_a</code> and <code>vec_b</code> are input C++ arrays and are initialized inside the next loop. <code>vec_c</code> is an output C++ array collecting computation results between <code>vec_a</code> and <code>vec_b</code>.</p> </li> <li> <p>lines 60-62 create buffers for each vector and specify their size. The runtime copies the data to the FPGA global memory when the kernel starts</p> </li> <li> <p>line 64 submits a command group to the device queue</p> </li> <li> <p>lines 66-68 relies on accessor to infer data dependencies. \"read_only\" accessor have to wait for data to be fetched. \"no_init\" option indicates ito the runtime know that the previous contents of the buffer can be discarded</p> </li> <li> <p>lines 70-73 starts a single tasks (single work-item) and call the kernel function</p> </li> <li> <p>lines 99-105 catch SYCL exceptions and terminate the execution</p> </li> </ul>"},{"location":"compile/#emulation","title":"Emulation","text":"<ul> <li> <p>FPGA emulation refers to the process of using a software or hardware system to mimic the behavior of an FPGA device. This is usually done to test, validate, and debug FPGA designs before deploying them on actual hardware. The Intel\u00ae FPGA emulator runs the code on the host cpu.</p> </li> <li> <p>Emulation is crucial to validate the functionality of your kernel design. </p> </li> <li> <p>During emulation, your are not seeking for performance.</p> </li> </ul> <p>Compile for emulation (in one step)</p> <pre><code>icpx -fsycl -fintelfpga -qactypes vector_add.cpp -o vector_add.fpga_emu\n</code></pre> <p>Intel uses the SYCL Ahead-of-time (AoT) compilation which as two steps:</p> <ol> <li> <p>The \"compile\" stage compiles the device code to an intermediate representation (SPIR-V).</p> </li> <li> <p>The \"link\" stage invokes the compiler's FPGA backend before linking.</p> </li> </ol> <p>Two-steps compilation</p> <pre><code># Compile \nicpx -fsycl -fintelfpga -qactypes -o vector_add.cpp.o -c vector_add.cpp\n# Link\nicpx -fsycl -fintelfpga -qactypes vector_add.cpp.o -o vector_add.fpga_emu\n</code></pre> <ul> <li>The compiler option <code>-qactypes</code> informs the compiler to sreahc and include the Algorithmic C (AC) data type folder for header and libs to the AC data types libraries for Field Programmable Gate Array (FPGA) and CPU compilations.</li> <li>The Algorithmic C (AC) datatypes libraries include a numerical set of datatypes and an interface datatype for modeling channels in communicating processes in C++.</li> </ul>"},{"location":"compile/#static-reports","title":"Static reports","text":"<ul> <li> <p>During the process of compiling an FPGA hardware image with the Intel\u00ae oneAPI DPC++/C++ Compiler, various checkpoints are provided at different compilation steps. These steps include object files generation, an FPGA early image object generation, an FPGA image object generation, and finally executables generation. These checkpoints offer the ability to review errors and make modifications to the source code without needing to do a full compilation every time. </p> </li> <li> <p>When you reach the FPGA early image object checkpoint, you can examine the optimization report generated by the compiler. </p> </li> <li> <p>Upon arriving at the FPGA image object checkpoint, the compiler produces a finished FPGA image.</p> </li> </ul> <p>In order to generate the FPGA early image, you will need to add the following option:</p> <ul> <li> <p><code>-Xshardware</code></p> </li> <li> <p><code>-Xstarget=&lt;target&gt;</code> or <code>-Xsboard=&lt;board&gt;</code></p> </li> <li> <p><code>-fsycl-link=early</code></p> </li> </ul> <p>Compile for FPGA early image</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xsboard=p520_hpc_m210h_g3x16 vector_add.cpp -o vector_add_report.a\n</code></pre> <ul> <li> <p>The <code>vector_add_report.a</code> is not what we target in priority. We target the reports directory <code>vector_add_report.prj</code> which has been created.</p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code>"},{"location":"compile/#full-compilation","title":"Full compilation","text":"<p>This phase produces the actual FPGA bitstream, i.e., a file containing the programming data associated with your FPGA chip. This file requires the target FPGA platform to be generated and executed. For FPGA programming, the Intel\u00ae oneAPI toolkit requires the Intel\u00ae Quartus\u00ae Prime software to generate this bitstream.</p> <p>Full hardware compilation</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE vector_add.cpp -o vector_add_report.fpga\n</code></pre> <ul> <li> <p>The compilation will take several hours. Therefore, we strongly advise you to verify your code through emulation first.</p> </li> <li> <p>You can also use the <code>-Xsfast-compile</code> option which offers a faster compile time but reduce the performance of the final FPGA image.</p> </li> </ul>"},{"location":"compile/#fast-recompilation","title":"Fast recompilation","text":"<ul> <li> <p>At first glance having a single source file is not necessarily a good idea when host and device compilation differs so much</p> </li> <li> <p>However, there is two different strategies to deal with it:</p> </li> <li> <p>Use a single source file and add the <code>-reuse-exe</code></p> </li> <li> <p>Separate host and device code compilation in your FPGA project</p> </li> <li> <p>This is up to you to choose the method that suits you the most</p> </li> </ul> <p>Using the <code>-reuse-exe</code> option</p> <p><pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE -reuse-exe=vector_add.fpga vector_add.cpp -o vector_add.fpga\n</code></pre> If only the host code changed since the previous compilation, providing the <code>-reuse-exe=image</code> flag to <code>icpx</code> instructs the compiler to extract the compiled FPGA binary from the existing executable and package it into the new executable, saving the device compilation time.</p> <p>Question</p> <ul> <li>What happens if the vector_add.fpga is missing ?</li> </ul> <p>Separating host and device code</p> <p>Go to the <code>GettingStarted/fpga_recompile</code> folder. It provides an example of separate host and device code The process is similar as the compilation process for OpenCL except that a single tool is used, i.e., <code>icpx</code></p> <ol> <li>Compile the host code: <pre><code>icpx -fsycl -fintelfpga -DFPGA_HARDWARE host.cpp -c -o host.o\n</code></pre></li> <li>Compile the FPGA image: <pre><code>icpx -fsycl -fintelfpga -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -fsycl-link=image kernel.cpp -o dev_image.a\n</code></pre></li> <li>Link both: <pre><code>icpx -fsycl -fintelfpga host.o dev_image.a -o fast_recompile.fpga\n</code></pre></li> </ol>"},{"location":"dpcpp/","title":"Introduction to FPGA programming with Intel\u00ae oneAPI","text":"<p>Intel\u00ae oneAPI is a software development toolkit from Intel designed to simplify the process of developing high-performance applications for various types of computing architecture. It aims to provide a unified and simplified programming model for CPUs, GPUs, FPGAs, and other types of hardware, such as AI accelerators, allowing developers to use a single codebase for multiple platforms.</p> <p>One of the main components of oneAPI is the Data Parallel C++ (DPC++), an open, standards-based language built upon the ISO C++ and SYCL standards. DPC++ extends C++ with features like parallel programming constructs and heterogeneous computing support, providing developers with the flexibility to write code for different types of hardware with relative ease.</p> <p>In addition to DPC++, oneAPI includes a range of libraries designed to optimize specific types of tasks, such as machine learning, linear algebra, and deep learning. These include oneDNN for deep neural networks, oneMKL for math kernel library, and oneDAL for data analytics, among others.</p> <p>It's important to note that Intel oneAPI is part of Intel's broader strategy towards open, standards-based, cross-architecture programming, which is intended to reduce the complexity of application development and help developers leverage the capabilities of different types of hardware more efficiently and effectively.</p> <p>In this documentation, you will explore how to:</p> <ul> <li>Use the DPC++ compiler to create executable for Intel FPGA hardware</li> <li>Discover the SYCL C++ abstraction layer</li> <li>How to move data from and to FPGA hardware</li> <li>Optimize FPGA workflows</li> </ul> <p>In order to get an overview of FPGA computing for the HPC ecosystem, please refer to the following slides.</p>"},{"location":"dpcpp/#what-is-the-intel-oneapi-dpc-compiler","title":"What is the Intel\u00ae oneAPI DPC++ compiler","text":"<p>In heterogenous computing, accelerator devices support the host processor by executing specific portion of code more efficiently. In this context, the Intel\u00ae oneAPI toolkit supports two different approaches for heterogeous computing:</p> <p>1. Data Parallel C++ with SYCL</p> <p>SYCL (Specification for Unified Cross-platform C++) provides a higher-level model for writing standard ISO C++ code that is both performance-oriented and portable across various hardware, including CPUs, GPUs and FPGAs It enables the use of standard C++ with extensions to leverage parallel hardware. Host and kernel code share the same source file. The DPC++ compiler is adding SYCL support on top of the LLVM C++ compiler. DPC++ is distributed with the Intel\u00ae oneAPI toolkit.</p> <p>2. OpenMP for C, C++, and Fortran </p> <p>For more than two decades, OpenMP has stood as a standard programming language, with Intel implementing its 5th version. The Intel oneAPI C++ Compiler, which includes support for OpenMP offloading, can be found in the Intel oneAPI Base Toolkit, Intel oneAPI HPC Toolkit, and Intel oneAPI IoT Toolkit. Both the Intel\u00ae Fortran Compiler Classic and the Intel\u00ae Fortran Compiler equipped with OpenMP offload support are accessible through the Intel oneAPI HPC Toolkit. </p> <p>Note: OpenMP is not supported for FPGA devices.</p>"},{"location":"dpcpp/#dpc-is-one-of-the-existing-sycl-implementations","title":"DPC++ is one of the existing SYCL implementations","text":"<p>ComputeCpp (codeplay)</p> <p>Support for ComputeCpp will no longer be provided from September 1st 2023 (see announce)</p>"},{"location":"dpcpp/#key-features-and-components","title":"Key Features and Components","text":"<ul> <li>Heterogeneous Support: Enables coding for various types of processors within the same program.</li> <li>Performance Optimization: It offers various optimization techniques to ensure code runs as efficiently as possible.</li> <li>Standard Compliance: Aligns with the latest C++ standards, along with the SYCL standard.</li> <li>Debugging and Analysis Tools: Integrates with tools that assist in debugging and analyzing code.</li> <li>Integration with IDEs: Compatible with popular Integrated Development Environments to facilitate a seamless coding experience.</li> <li>Open Source and Community Driven: This promotes collaboration and ensures that the technology stays up to date with industry needs.</li> </ul>"},{"location":"dpcpp/#sycl-and-fpga","title":"SYCL and FPGA","text":"<p>SYCL offers APIs and abstractions, but FPGA cards are unique to each vendor, and even within the same vendor, FPGA cards may have diverse capabilities. DPC++ targets Intel\u00ae FPGA cards specifically and extends SYCL's functions. This allows it to leverage the strength of FPGA, all the while maintaining as much generalizability and portability as possible.</p>"},{"location":"dpcpp/#references","title":"References","text":"<ul> <li>Data Parallel C++: Mastering DPC++ for Programming of Heterogeneous Systems using C++ and SYCL</li> <li>Intel\u00ae oneAPI DPC++/C++ Compiler </li> <li>SYCL official documentation</li> </ul>"},{"location":"intro/","title":"Introduction to FPGA computing for the HPC ecosystem","text":""},{"location":"intro/#field-programmable-gate-array-fpga","title":"Field Programmable Gate Array (FPGA)","text":"<p>An FPGA (Field-Programmable Gate Array) is an integrated circuit designed to be configured by the user after manufacturing. It consists of an array of programmable logic blocks and a hierarchy of reconfigurable interconnects, allowing users to create custom digital circuits. FPGAs are known for their flexibility, enabling rapid prototyping and implementation of complex functions in hardware, making them suitable for applications in telecommunications, automotive, aerospace, and various other fields where custom and high-performance computing is needed.</p> <p>Difference Between FPGA Development Boards and HPC FPGA Cards</p> FPGA Development BoardsHPC FPGA Cards <ul> <li>Purpose: Primarily used for prototyping, learning, and development purposes.</li> <li>Design: These boards typically come with various interfaces (like HDMI, USB, Ethernet) and peripherals (like buttons, LEDs, and sensors) to facilitate easy testing and development.</li> <li>Flexibility: They offer a broad range of input/output options to support diverse projects and experiments.</li> <li>Cost: Generally more affordable than HPC FPGA cards due to their focus on versatility and accessibility.</li> <li>Target Audience: Suitable for students, hobbyists, and engineers working on initial project phases or small-scale applications.</li> <li>Specifications: <ul> <li>Logic Cells: 33,280 </li> <li>Block RAM: 1,800 Kbits</li> <li>External memory: None</li> </ul> </li> </ul> <p></p> <ul> <li>Purpose: Designed for high-performance computing (HPC) applications, focusing on accelerating compute-intensive tasks.</li> <li>Design: Typically more powerful, with higher logic capacity, memory, and bandwidth capabilities. They often come with specialized cooling solutions and are designed to be mounted in server racks.</li> <li>Performance: Optimized for tasks such as data center operations, machine learning, financial modeling, and large-scale scientific computations.</li> <li>Cost: Generally more expensive due to their advanced features and high-performance capabilities.</li> <li>Target Audience: Aimed at professionals in industries requiring significant computational power, such as data scientists, researchers, and engineers in high-performance computing sectors.</li> <li>Specifications:<ul> <li>Logic Cells: 2,073,000 </li> <li>Block RAM: 239.5 Mb</li> <li>External memory: 16GB HBM2</li> </ul> </li> </ul> <p></p>"},{"location":"intro/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<ul> <li>FPGA strongly differs from ISA-based hardware such as CPU and GPU</li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize the everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p></p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Pipelining example provided chap.17 (DPC++ book)</p> Processing a single element (Figure. 17-13)Taking advantage of pipelining (Figure 17-14) <p></p> <ul> <li>The pipeline is mostly empty.</li> <li>Hardware units are not busy and the efficiency is thus low.</li> </ul> <p></p> <ul> <li>More data than stages, the pipeline is full and all hardware units are busy.</li> </ul> <p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"intro/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of thread started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16"},{"location":"intro/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the programmer need not do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul>"},{"location":"intro/#meluxina-bittware-520n-mx-fpgas","title":"MeluXina Bittware 520N-MX FPGAS","text":"<p>Each of the 20 MeluXina FPGA compute nodes comprise two BittWare 520N-MX FPGAs based on the Intel Stratix 10 FPGA chip. Designed for compute acceleration, the 520N-MX are PCIe boards featuring Intel\u2019s Stratix 10 MX2100 FPGA with integrated HBM2 memory. The size and speed of HBM2 (16GB at up to 512GB/s) enables acceleration of memory-bound applications. Programming with high abstraction C, C++, and OpenCLis possible through an specialized board support package (BSP) for the Intel OpenCL SDK. For more details see the dedicated BittWare product page.</p> <p> <p>Intel Stratix 520N-MX Block Diagram. </p> <p>The Bittware 520N-MX cards have the following specifications:</p> <ol> <li>FPGA: Intel Stratix 10 MX with MX2100 in an F2597 package, 16GBytes on-chip High Bandwidth Memory (HBM2) DRAM, 410 GB/s (speed grade 2).</li> <li>Host interface:    x16 Gen3 interface direct to FPGA, connected to PCIe hard IP.</li> <li>Board Management Controller<ul> <li>FPGA configuration and control</li> <li>Voltage, current, temperature monitoring</li> <li>Low bandwidth BMC-FPGA comms with SPI link</li> </ul> </li> <li>Development Tools<ul> <li>Application development: supported design flows - Intel FPGA OpenCL SDK, Intel High-Level Synthesis (C/C++) &amp; Quartus Prime Pro (HDL, Verilog, VHDL, etc.)</li> <li>FPGA development BIST - Built-In Self-Test with source code (pinout, gateware, PCIe driver &amp; host test application)</li> </ul> </li> </ol> <p>The FPGA cards are not directly connected to the MeluXina ethernet network. The FPGA compute nodes are linked into the high-speed (infiniband) fabric, and the host code can communicate over this network for distributed/parallel applications.</p> <p>More details on FPGA can be found in the presentation below:</p> <p></p>"},{"location":"oneapi_quantum/","title":"Developing Quantum Circuit","text":"<ul> <li>We will develop the quantum circuit we have seen previously, i.e.,  Bernstein-Varizani to search an hidden element</li> <li>We will need to code the Pauli H and Z gates </li> <li>We will also need a function to measure (sample) from the distribution obtained using the state vector amplitudes</li> </ul>"},{"location":"oneapi_quantum/#setup","title":"Setup","text":"<ul> <li>We need first to obtain an interactive job on the fpga partition and load some modules</li> </ul> <p>Commands</p> <pre><code># Get one FPGA node with two FPGA cards\nsalloc -A &lt;ACCOUNT&gt; -t 02:00:00 -q default -p fpga -N1\nmodule load env/staging/2023.1\nmodule load git-lfs\nmodule load CMake intel-fpga 520nmx\n</code></pre> <ul> <li> <p>Clone the repository if not already done: <code>git lfs clone https://github.com/LuxProvide/QuantumFPGA</code></p> </li> <li> <p>Create a symbolic link to the project <code>ln -s QuantumFPGA/code/FQSim</code> and <code>cd FQSim</code></p> </li> <li> <p>The project contains the following files:</p> </li> </ul> <pre><code>$&gt;tree\n.\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 fpga_image\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 quantum.fpga\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bernstein-vazirani.cpp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kernels.cpp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kernels.hpp\n\u2514\u2500\u2500 src-solution\n    \u251c\u2500\u2500 bernstein-vazirani.cpp\n    \u251c\u2500\u2500 kernels.cpp\n    \u2514\u2500\u2500 kernels.hpp\n</code></pre> <ul> <li>fpga_image : contains the fpga image build prior to the workshop training to avoid waiting hardware synthesis. Indeed, the offline compiler will extract the bitstream file <code>aocx</code> and reuse it if only if the device code did not change</li> <li>src : All files contain blank code that we are going to fill step by step<ul> <li>bernstein-vazirani.cpp: the source file with the Bernstein-Vazirani circuit.</li> <li>kernel.cpp: the source file containing all code for the gates.</li> <li>kernel.hpp: the header file containing the signature of function.</li> </ul> </li> <li>src-solution:  the solution to fill all blank code. Replace <code>set(SOURCE_FILES src/bernstein-vazirani.cpp src/kernels.cpp)</code> by <code>set(SOURCE_FILES src-solution/bernstein-vazirani.cpp src-solution/kernels.cpp)</code> in the CMakeLists.txt file</li> </ul>"},{"location":"oneapi_quantum/#building-code","title":"Building code","text":"<ul> <li>We strongly recommend to compile and execute your code using the <code>Intel(R) FPGA Emulation Platform</code> which does not require any FPGA board on the system.</li> </ul> <p>Commands</p> <pre><code>mkdir build-emu &amp;&amp; cd build-emu\ncmake ..\nmake fpga_emu\n</code></pre> <ul> <li>Once your code runs on the <code>Intel(R) FPGA Emulation Platform</code> without errors:</li> </ul> <p>Commands</p> <pre><code>mkdir build-fpga &amp;&amp; cd build-fpga\ncmake ..\nmake fpga\n</code></pre> <p>Using Direct Memory Access (DMA)</p> <ul> <li>DMA is enabled between host and the device if buffer data have a 64-byte alignment.</li> <li>We strongly recommend you to load our <code>jemalloc</code> module which provides such default alignment: <pre><code>module load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./exe\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#device-code","title":"Device code","text":"<ul> <li> <p>The highlighted code corresponds to the device code or kernel code running on the device</p> </li> <li> <p>Device code is mainly used to modify or accelerate operation related to the state vector. We are going to explain it in a few minutes.</p> </li> <li> <p>We are not going to modify this code but we will use it to create the two required gates</p> </li> </ul> kernels.cpp<pre><code>#include \"kernels.hpp\"\n#include &lt;bitset&gt;\n#include &lt;random&gt;\n#include &lt;algorithm&gt;\n#include &lt;numeric&gt;\n#include &lt;string&gt;\nstd::string toBinary(int num,int numQubits) {\nstd::string result;\nfor (int i = numQubits - 1; i &gt;= 0; i--) {\nint bit = (num &gt;&gt; i) &amp; 1;\nresult += std::to_string(bit);\n}\nreturn result;\n}\nint nth_cleared(int n, int target)\n{\nint mask = (1 &lt;&lt; target) - 1;\nint not_mask = ~mask;\nreturn (n &amp; mask) | ((n &amp; not_mask) &lt;&lt; 1);\n}\nvoid apply_gate(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates, const int target, const std::complex&lt;float&gt; A,\nconst std::complex&lt;float&gt; B,\nconst std::complex&lt;float&gt; C,\nconst std::complex&lt;float&gt; D)\n{\nqueue.parallel_for&lt;class Gate&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\nint global_id = item.get_id(0);\nconst int zero_state = nth_cleared(global_id,target);\nconst int one_state = zero_state | (1 &lt;&lt; target);\nstd::complex&lt;float&gt; zero_amp = stateVector_d[zero_state];\nstd::complex&lt;float&gt; one_amp = stateVector_d[one_state];\nstateVector_d[zero_state] = A * zero_amp + B * one_amp;\nstateVector_d[one_state] =  C * zero_amp + D * one_amp;\n}).wait();\n}\nvoid h(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numQubits,const int target){\n/*\n    Code for the Pauli X gate here\n    */\n}\nvoid z(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\nconst unsigned int numQubits,\nconst int target){\n/*\n    Put the code for the Pauli Z gate here\n    */\n}\nvoid get_proba(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates,float *probaVector_d){\nqueue.parallel_for&lt;class Proba&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\nint global_id = item.get_id(0);\nstd::complex&lt;float&gt; amp = stateVector_d[global_id];\nprobaVector_d[global_id] = std::abs(amp * amp);\n}).wait();\n}\nvoid measure(sycl::queue &amp;queue,std::complex&lt;float&gt; *stateVector_d, int numQubits,int samples){\n/*\n    Put the code to measure (sample) here\n    */\n}\n</code></pre>"},{"location":"oneapi_quantum/#general-approach-to-apply-a-quantum-gate","title":"General approach to apply a quantum gate","text":"<ul> <li> <p>Let's consider a multiqubit register $ |\\psi \\rangle = \\sum\\limits_{i=0}^{2^N-1} \\alpha_i |i \\rangle $ which is a mixture of pure state</p> </li> <li> <p>$ |i \\rangle $ is the decimal representation of the pure state i </p> </li> <li> <p>As you can observe it, the number of pure states constituting the state vector is growing exponentially. </p> </li> <li> <p>We can take advantage of the FPGA to apply gate in parallel </p> </li> </ul> kernel code<pre><code>int nth_cleared(int n, int target)\n{\nint mask = (1 &lt;&lt; target) - 1;\nint not_mask = ~mask;\nreturn (n &amp; mask) | ((n &amp; not_mask) &lt;&lt; 1);\n}\nvoid apply_gate(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates, const int target, const std::complex&lt;float&gt; A,\nconst std::complex&lt;float&gt; B,\nconst std::complex&lt;float&gt; C,\nconst std::complex&lt;float&gt; D)\n{\nqueue.parallel_for&lt;class Gate&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\nint global_id = item.get_id(0);\nconst int zero_state = nth_cleared(global_id,target);\nconst int one_state = zero_state | (1 &lt;&lt; target);\nstd::complex&lt;float&gt; zero_amp = stateVector_d[zero_state];\nstd::complex&lt;float&gt; one_amp = stateVector_d[one_state];\nstateVector_d[zero_state] = A * zero_amp + B * one_amp;\nstateVector_d[one_state] =  C * zero_amp + D * one_amp;\n}).wait();\n}\n</code></pre> <p>Example:  applying a general U gate to qubit 2:</p> <ul> <li> <p>let's consider \\(U=\\begin{pmatrix} u_1 &amp; u_2 \\\\ u_3 &amp; u_4 \\end{pmatrix}\\)</p> </li> <li> <p>Apply \\(U\\) on qubit 2 is $ I \\otimes U \\otimes I $ with $ I $ being the identity matrix</p> </li> </ul> \\[ \\begin{aligned} I \\otimes U \\otimes I|\\psi_1 \\psi_2 \\psi_3 \\rangle &amp; =  |\\psi_1 \\rangle &amp; \\otimes &amp; \\hspace{2em} U|\\psi_2 \\rangle  &amp; \\otimes &amp; | \\psi_3 \\rangle \\\\                               &amp; = \\begin{pmatrix} \\alpha_1  \\\\ \\beta_1 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} u_1 &amp; u_2 \\\\ u_3 &amp; u_4 \\end{pmatrix} \\begin{pmatrix} \\alpha_2  \\\\ \\beta_2 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} \\alpha_3  \\\\ \\beta_3 \\end{pmatrix} \\\\                               &amp; = \\begin{pmatrix} \\alpha_1  \\\\ \\beta_1 \\end{pmatrix} &amp; \\otimes &amp;  \\begin{pmatrix} u_1 \\alpha_2 + u_2 \\beta_2 \\\\ u_3 \\alpha_2  + u_4 \\beta_2 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} \\alpha_3  \\\\ \\beta_3 \\end{pmatrix} \\\\ \\end{aligned} \\] \\[ \\begin{aligned}                               \\hspace*{1cm} &amp; = \\begin{matrix}                                                ( \\alpha_1 { \\color{red}{       u_1 \\alpha_2 + u_2 \\beta_2 }}  \\alpha_3)  |000 \\rangle + \\\\                                               ( \\alpha_1 { \\color{blue}{     u_1 \\alpha_2 + u_2 \\beta_2  }}  \\beta_3  )  |001 \\rangle + \\\\                                                ( \\alpha_1 { \\color{red}{       u_3 \\alpha_2  + u_4 \\beta_2}}  \\alpha_3 )  |010 \\rangle + \\\\                                               ( \\alpha_1 { \\color{blue}{     u_3 \\alpha_2  + u_4 \\beta_2 }}  \\beta_3  )  |011 \\rangle + \\\\                                               ( \\beta_1  { \\color{green}{   u_1 \\alpha_2 + u_2 \\beta_2   }}  \\alpha_3 )  |100 \\rangle + \\\\                                               ( \\beta_1 { \\color{purple}{  u_1 \\alpha_2 + u_2 \\beta_2   }}  \\beta_3   )  |101 \\rangle + \\\\                                               ( \\beta_1 {  \\color{green}{  u_3 \\alpha_2  + u_4 \\beta_2  }}  \\alpha_3  )  |110 \\rangle + \\\\                                               ( \\beta_1{  \\color{purple}{ u_3 \\alpha_2  + u_4 \\beta_2  }}  \\beta_3    )  |111 \\rangle \\phantom{0}                                  \\end{matrix} \\end{aligned} \\] <ul> <li> <p>Nonetheless, we are not applying the tensor product \\(\\otimes\\) every time which would be inefficient </p> </li> <li> <p>Starting from specific state vector, we will apply for example a gate to the 2nd qubit like following:</p> </li> </ul> \\[ \\begin{aligned}                                \\begin{pmatrix} \\alpha_1 {\\color{red}{\\alpha_2}   } \\alpha_3 \\\\                                                \\alpha_1 { \\color{blue}{\\alpha_2} } \\beta_3  \\\\                                                \\alpha_1 { \\color{red}{\\beta_2}   } \\alpha_3  \\\\                                                \\alpha_1 { \\color{blue}{\\beta_2}  } \\beta_3   \\\\                                                 \\beta_1  {\\color{green}{\\alpha_2} } \\alpha_3  \\\\                                                \\beta_1  {\\color{purple}{\\alpha_2}}  \\beta_3   \\\\                                                \\beta_1  {\\color{green}{\\beta_2}  }  \\alpha_3   \\\\                                                \\beta_1  {\\color{purple}{\\beta_2} }  \\beta_3    \\\\                                \\end{pmatrix}                               &amp; \\rightarrow \\begin{pmatrix}                                \\alpha_1 { \\color{red}{       u_1 \\alpha_2 + u_2 \\beta_2 }}  \\alpha_3  \\\\                               \\alpha_1 { \\color{blue}{     u_1 \\alpha_2 + u_2 \\beta_2  }}  \\beta_3 \\\\                                \\alpha_1 { \\color{red}{       u_3 \\alpha_2  + u_4 \\beta_2}}  \\alpha_3  \\\\                                \\alpha_1 { \\color{blue}{     u_3 \\alpha_2  + u_4 \\beta_2 }}  \\beta_3  \\\\                                \\beta_1  { \\color{green}{   u_1 \\alpha_2 + u_2 \\beta_2   }}  \\alpha_3  \\\\                                 \\beta_1 { \\color{purple}{  u_1 \\alpha_2 + u_2 \\beta_2   }}  \\beta_3  \\\\                                \\beta_1 {  \\color{green}{  u_3 \\alpha_2  + u_4 \\beta_2  }}  \\alpha_3  \\\\                                 \\beta_1{  \\color{purple}{ u_3 \\alpha_2  + u_4 \\beta_2  }}  \\beta_3                                 \\end{pmatrix}  \\end{aligned} \\] <ul> <li> <p>As you can see in the previous example to apply a gate U with its 4 complex coefficient, we apply \\((u_1 u_2)\\) to the coefficients corresponding to basis vector with a 0 at position 2 and \\((u_3 u_4)\\) to the coefficients corresponding to basis vector with a 1 at position 2</p> </li> <li> <p>Knowing this fact, we can divide by two the search and apply the gate coefficient by only searching the 1st, 2nd, kth number where the basis vector has a 0 at the chosen position</p> </li> <li> <p>To convince you, let's continue with our previous example:</p> </li> </ul> \\[ \\begin{aligned} \\begin{matrix}  |000 \\rangle &amp; \\rightarrow \\text{1st (index 0) position with 0 at position 2}  \\\\ |001 \\rangle &amp; \\rightarrow \\text{2nd (index 1) position with 0 at position 2}  \\\\  |010 \\rangle &amp; \\\\ |011 \\rangle &amp; \\\\ |100 \\rangle &amp; \\rightarrow \\text{3rd (index 2) position with 0 at position 2} \\\\ |101 \\rangle &amp; \\rightarrow \\text{4th (index 3) position with 0 at position 2}\\\\ |110 \\rangle &amp; \\\\ |111 \\rangle &amp; \\phantom{0}   \\end{matrix} \\end{aligned} \\] <ul> <li>Starting from the indexes, we can find where we should apply \\((u_1 u_2)\\) coefficient</li> </ul> \\[ \\begin{aligned} \\begin{matrix}  00  &amp; \\rightarrow \\text{index 0. Adding 0 to position 2} &amp; \\rightarrow   |000 \\rangle   \\\\ 01  &amp; \\rightarrow \\text{index 1. Adding 0 to position 2} &amp; \\rightarrow   |001 \\rangle  \\\\  10  &amp; \\rightarrow \\text{index 2.  Adding 0 to position 2}&amp; \\rightarrow   |100 \\rangle    \\\\ 11  &amp; \\rightarrow \\text{index 3. Adding 0 to position 2} &amp; \\rightarrow   |101 \\rangle    \\\\ \\end{matrix} \\end{aligned} \\] <ul> <li> <p>To find coeffcients where \\((u_3 u_4)\\) should be applied, we only need to add 1 instead of 0 to get the corresponding basis vector</p> </li> <li> <p>Finally, we will have two functions to apply any kind of one qubit gate (except the controlled gates):</p> <ul> <li>nth_cleared: finds the Nth number where a given binary digit is set to 0. </li> <li>apply_gate:  apply a general one qubit gate by finding in parallel all pure vector with digit 2 set to 0. For each of these vector, we can easily find the one with digit set to 1 and replace the amplitudes according to what we have above </li> </ul> </li> </ul>"},{"location":"oneapi_quantum/#computing-probabilities-from-state-vector-amplitudes","title":"Computing probabilities from state vector amplitudes","text":"<ul> <li> <p>The following kernel is only used to compute the probability for each pure state vector</p> </li> <li> <p>We compute that on the FPGA card as its is time-consuming</p> </li> </ul> kernel code<pre><code>queue.parallel_for&lt;class Proba&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\nint global_id = item.get_id(0);\nstd::complex&lt;float&gt; amp = stateVector_d[global_id];\nprobaVector_d[global_id] = std::abs(amp * amp);\n}).wait();\n</code></pre>"},{"location":"oneapi_quantum/#measuring-qubits","title":"Measuring qubits:","text":"<ul> <li> <p>Quantum State Collapse: In classical simulation, measurements typically do not affect the system being measured. However, in quantum simulations, the act of measurement causes the qubit to collapse from its superposition of states to one of the basis states (e.g., 0 or 1). This is a fundamental aspect of quantum mechanics known as wave function collapse.</p> </li> <li> <p>Probabilistic Nature: The result of measuring a qubit is probabilistic. Before measurement, a qubit in superposition has probabilities associated with its possible states. Measurement forces the qubit into one of these states, with the likelihood of each state given by its quantum amplitude squared.</p> </li> <li> <p>In simulation, measuring is a synonym of sampling </p> </li> </ul> <p>Sampling the possible outcomes</p> QuestionSolution <ul> <li>Using the <code>void get_proba(...)</code> function, fill the body of the <code>void measure(...)</code> function</li> <li>You can use the standard library function <code>std::discrete_distribution</code> (see below) <pre><code>std::random_device rd;                          // Obtain a random number from hardware\nstd::mt19937 gen(rd());                         // Seed the generator\nstd::discrete_distribution&lt;&gt; dist(probaVector, probaVector + size);\n</code></pre></li> </ul> <ul> <li>Add the following code in the <code>void measure(...)</code> function body <pre><code> int size = std::pow(2,numQubits);\nfloat *probaVector = new float[size];\nfloat *probaVector_d = malloc_device&lt;float&gt;(size,queue);\nget_proba(queue,stateVector_d,size,probaVector_d); queue.memcpy(probaVector, probaVector_d, size * sizeof(float)).wait();\nstd::random_device rd;                          // Obtain a random number from hardware\nstd::mt19937 gen(rd());                         // Seed the generator\nstd::discrete_distribution&lt;&gt; dist(probaVector, probaVector + size);\nstd::vector&lt;int&gt; arr(size);\nfor(int i = 0; i &lt; samples; i++){\nint index = dist(gen);\narr[index]++;\n}\nstd::cout &lt;&lt; \"Quantum State Probabilities:\" &lt;&lt; std::endl;\nfor (size_t i = 0; i &lt; size; ++i) {\nstd::cout &lt;&lt; \"State \" &lt;&lt;toBinary(i,numQubits) &lt;&lt; \": \" &lt;&lt; arr[i] &lt;&lt; std::endl;\n}\ndelete[] probaVector;\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#implementing-the-two-pauli-h-and-z-gates","title":"Implementing the two Pauli H and Z gates","text":"<p>Pauli H gate</p> QuestionSolution <ul> <li>The Hadamard gate puts qubits in superposition</li> <li>It transform the basis state:<ul> <li>\\(|0 \\rangle\\) to $\\frac{|0\\rangle + |1 \\rangle}{\\sqrt{2}} $</li> <li>\\(|1 \\rangle\\) to $\\frac{|0\\rangle - |1 \\rangle}{\\sqrt{2}} $</li> </ul> </li> </ul> <p> \\(\\begin{aligned} H &amp; = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 &amp; 1 \\\\1 &amp; -1 \\end{pmatrix}\\end{aligned}\\) </p> <ul> <li> <p>Fill the body of the <code>void h(...)</code> function body</p> </li> <li> <p>To test your gate, set the variable <code>SOURCE_FILES</code> as follows <code>set(SOURCE_FILES src/test_h_gate.cpp src/kernels.cpp)</code> in the CMakeLists.txt file</p> </li> </ul> <p>Building and the code</p> <pre><code>mkdir build-test-h-gate &amp;&amp; cd build-test-h-gate\ncmake ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre> <ul> <li>Add the following code in the <code>void h(...)</code> function body <pre><code>std::complex&lt;float&gt; A (1.0f,0.0f);\nstd::complex&lt;float&gt; B (1.0f,0.0f);\nstd::complex&lt;float&gt; C (1.0f,0.0f);\nstd::complex&lt;float&gt; D (-1.0f,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A/std::sqrt(2.0f),\nB/std::sqrt(2.0f),\nC/std::sqrt(2.0f),\nD/std::sqrt(2.0f));\n</code></pre></li> </ul> <p>Pauli Z gate</p> DescriptionSolution <p>The Pauli-Z gate is a single-qubit rotation through \\(\\pi\\) radians around the z-axis.  \\(\\begin{aligned} Z &amp; = \\begin{pmatrix}1 &amp; 0 \\\\0 &amp; -1 \\end{pmatrix}\\end{aligned}\\) </p> <ul> <li> <p>Fill the body of the <code>void z(...)</code> function body</p> </li> <li> <p>To test your gate, set the variable <code>SOURCE_FILES</code> as follows <code>set(SOURCE_FILES src/test_z_gate.cpp src/kernels.cpp)</code> in the CMakeLists.txt file</p> </li> </ul> <p>Building and the code</p> <pre><code>mkdir build-test-z-gate &amp;&amp; cd build-test-z-gate\ncmake ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre> <ul> <li>Add the following code in the <code>void z(...)</code> function body <pre><code>std::complex&lt;float&gt; A (1.0f,0.0f);\nstd::complex&lt;float&gt; B (0.0f,0.0f);\nstd::complex&lt;float&gt; C (0.0f,0.0f);\nstd::complex&lt;float&gt; D (-1.0f,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\nB,\nC,\nD);\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#implementing-the-bernstein-varizani-algorithm","title":"Implementing the Bernstein-Varizani algorithm","text":"<p>Let's put everything together</p> QuestionSolution <ul> <li> <p>Fill the body of the <code>int main(...)</code> function body</p> </li> <li> <p>Apply the h gate to all qubits to put them all into superpositions</p> </li> <li> <p>Apply the z gate to the register qubits corresponding to the classical bits matching the so called hidden number</p> </li> <li> <p>Finally, sample from the probabilities using the <code>void measure(...)</code></p> </li> </ul> <p>Building and the code</p> <pre><code>mkdir build-BV &amp;&amp; cd build-BV\ncmake ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre> <pre><code>// Sample code Bernstein-Vazirani\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include \"kernels.hpp\"\nusing namespace sycl;\nint main() {\nbool passed = true;\ntry{\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// Create a SYCL queue\nqueue queue(selector);\n// make sure the device supports USM host allocations\nauto device = queue.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\nconstexpr size_t numQubits = 7;\nconstexpr size_t numStates = 1 &lt;&lt; numQubits; // 2^n\nconstexpr int hidden = 101; //Hidden integer\nstd::complex&lt;float&gt; *stateVector   = new std::complex&lt;float&gt;[numStates];\nstd::complex&lt;float&gt; *stateVector_d = malloc_device&lt;std::complex&lt;float&gt;&gt;(numStates,queue);\n// Initial state |00...00&gt;\nstateVector[0] = std::complex&lt;float&gt;(1.0f,0.0f);\nqueue.memcpy(stateVector_d, stateVector, numStates * sizeof(std::complex&lt;float&gt;)).wait();\n/*Code for the Bernstein-Vazirani circuit*/\nsycl::free(stateVector_d,queue);\n}catch (exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Building and the code</p> <pre><code>mkdir build-fpga &amp;&amp; cd build-fpga\ncmake ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre> <pre><code>// Sample code Bernstein-Vazirani\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include \"kernels.hpp\"\nusing namespace sycl;\nint main() {\nbool passed = true;\ntry{\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// Create a SYCL queue\nqueue queue(selector);\n// make sure the device supports USM host allocations\nauto device = queue.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\nconstexpr size_t numQubits = 7;\nconstexpr size_t numStates = 1 &lt;&lt; numQubits; // 2^n\nconstexpr int hidden = 101; //Hidden integer\nstd::complex&lt;float&gt; *stateVector   = new std::complex&lt;float&gt;[numStates];\nstd::complex&lt;float&gt; *stateVector_d = malloc_device&lt;std::complex&lt;float&gt;&gt;(numStates,queue);\n// Initial state |00...00&gt;\nstateVector[0] = std::complex&lt;float&gt;(1.0f,0.0f);\nqueue.memcpy(stateVector_d, stateVector, numStates * sizeof(std::complex&lt;float&gt;)).wait();\nfor(int i = 0; i &lt; numQubits; i++){\nh(queue, stateVector_d, numQubits,i);\n}\nfor(int j = 0; j &lt; numQubits; j++){\nif((hidden &amp; (1 &lt;&lt; j)) != 0){\nz(queue, stateVector_d, numQubits, j); }\n}\nfor(int i = 0; i &lt; numQubits; i++){\nh(queue, stateVector_d, numQubits,i);\n}\nmeasure(queue, stateVector_d, numQubits, 1000);\nsycl::free(stateVector_d,queue);\n}catch (exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"qcfpga/","title":"QES with QCFPGA on JupyterLab","text":"In\u00a0[1]: Copied! <pre>%%bash\nKERNEL=\"$HOME/.local/share/jupyter/kernels/qcfpga\"\nmkdir -p $KERNEL\nPRELOAD=\"$KERNEL/start.sh\"\nJSON=\"$KERNEL/kernel.json\"\ncat &lt;&lt; 'EOF' &gt; $JSON\n{\n \"argv\": [\n  \"{resource_dir}/start.sh\",\n  \"python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"QCFPGA\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\nEOF\n\ncat &lt;&lt; 'EOF' &gt; $PRELOAD\n#!/bin/bash\nmodule load QCFPGA\nmodule load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nexport LD_PRELOAD=${JEMALLOC_PRELOAD}\nexport PYOPENCL_COMPILER_OUTPUT=1\nexec \"$@\"\nEOF\n\nchmod u+x $PRELOAD\n</pre> %%bash KERNEL=\"$HOME/.local/share/jupyter/kernels/qcfpga\" mkdir -p $KERNEL PRELOAD=\"$KERNEL/start.sh\" JSON=\"$KERNEL/kernel.json\" cat &lt;&lt; 'EOF' &gt; $JSON {  \"argv\": [   \"{resource_dir}/start.sh\",   \"python\",   \"-m\",   \"ipykernel_launcher\",   \"-f\",   \"{connection_file}\"  ],  \"display_name\": \"QCFPGA\",  \"language\": \"python\",  \"metadata\": {   \"debugger\": true  } } EOF  cat &lt;&lt; 'EOF' &gt; $PRELOAD #!/bin/bash module load QCFPGA module load jemalloc export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision) export LD_PRELOAD=${JEMALLOC_PRELOAD} export PYOPENCL_COMPILER_OUTPUT=1 exec \"$@\" EOF  chmod u+x $PRELOAD  <ul> <li>Execute the following cell to reload everything</li> </ul> In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 <ul> <li>Now change the current kernel by the new one: <code>Kernel --&gt; Change Kernels --&gt; QCFPGA</code></li> <li>If you see a white circle, the kernel is ready</li> </ul> In\u00a0[1]: Copied! <pre># Import QCFPGA\nimport qcfpga\nimport numpy as np\n\n# Create a new quantum register with 1 qubits\nregister = qcfpga.State(1)\n# Let's try the Hadamard gate\nregister.h(0)\nnp.round(register.probabilities(),2)\n</pre> # Import QCFPGA import qcfpga import numpy as np  # Create a new quantum register with 1 qubits register = qcfpga.State(1) # Let's try the Hadamard gate register.h(0) np.round(register.probabilities(),2) <pre>/apps/USE/easybuild/release/2023.1/software/PyOpenCL/2023.1.4-foss-2023a-ifpgasdk-20.4/lib/python3.11/site-packages/pyopencl/__init__.py:528: CompilerWarning: From-binary build succeeded, but resulted in non-empty logs:\nBuild on &lt;pyopencl.Device 'p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)' on 'Intel(R) FPGA SDK for OpenCL(TM)' at 0x145395ce4898&gt; succeeded, but said:\n\nTrivial build\n  lambda: self._prg.build(options_bytes, devices),\n</pre> Out[1]: <pre>array([0.5, 0.5], dtype=float32)</pre> In\u00a0[2]: Copied! <pre>register = qcfpga.State(2)\n\nregister.h(0) # Applies the Hadamard  gate to the first qubit.\nregister.x(1) # Applies a pauli-x  gate to the second qubit.\nnp.round(register.probabilities(),2)\n</pre> register = qcfpga.State(2)  register.h(0) # Applies the Hadamard  gate to the first qubit. register.x(1) # Applies a pauli-x  gate to the second qubit. np.round(register.probabilities(),2) Out[2]: <pre>array([0. , 0. , 0.5, 0.5], dtype=float32)</pre> <p>These are the gates that can be applied to a register:</p> <ul> <li><p>The Hadamard gate: h - <code>state.h(0)</code></p> </li> <li><p>The S gate: s - <code>state.s(0)</code></p> </li> <li><p>The T gate: t - <code>state.t(0)</code></p> </li> <li><p>The Pauli-X / NOT gate: x - <code>state.x(0)</code></p> </li> <li><p>The Pauli-Y gate: y - <code>state.y(0)</code></p> </li> <li><p>The Pauli-Z gate: z - <code>state.z(0)</code></p> </li> <li><p>The CNOT gate: cx -<code>state.cx(0, 1) # CNOT with control = 0, target = 1</code></p> </li> <li><p>The SWAP gate: swap -<code>state.swap(0,1) # Swaps the 0th and 1st qubit</code></p> </li> <li><p>The Toffoli gate: toffoli -<code>state.toffoli(0, 1, 2) # Toffoli with controls = (0, 1), target = 2</code></p> </li> </ul> In\u00a0[3]: Copied! <pre>''' \nFor example, you can also use any of the gates as controlled gates. \nControlled gates can be also used to entangle state\n'''\nx = qcfpga.gate.x()\nh = qcfpga.gate.h()\n\nregister = qcfpga.State(5)\nregister.apply_gate(h,0)\nregister.apply_controlled_gate(x, 0, 1)\nnp.round(register.probabilities(),2)\n</pre> '''  For example, you can also use any of the gates as controlled gates.  Controlled gates can be also used to entangle state ''' x = qcfpga.gate.x() h = qcfpga.gate.h()  register = qcfpga.State(5) register.apply_gate(h,0) register.apply_controlled_gate(x, 0, 1) np.round(register.probabilities(),2) Out[3]: <pre>array([0.5, 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n       0. , 0. , 0. , 0. , 0. , 0. ], dtype=float32)</pre> In\u00a0[4]: Copied! <pre>'''\nIt is also trivial to apply a gate to all qubit of a register\n'''\nh = qcfpga.gate.h()\n\nregister = qcfpga.State(3)\nregister.apply_all(h)\nnp.round(register.probabilities(),3)\n</pre> ''' It is also trivial to apply a gate to all qubit of a register ''' h = qcfpga.gate.h()  register = qcfpga.State(3) register.apply_all(h) np.round(register.probabilities(),3)  Out[4]: <pre>array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125],\n      dtype=float32)</pre> In\u00a0[5]: Copied! <pre>gate_matrix = np.array([\n    [np.cos(np.pi/6), -np.sin(np.pi/6)],\n    [np.sin(np.pi/6),np.cos(np.pi/6)]\n])\n\ngate = qcfpga.Gate(gate_matrix)\nregister = qcfpga.State(3)\nregister.apply_all(gate)\nnp.round(register.probabilities(),3)\n</pre> gate_matrix = np.array([     [np.cos(np.pi/6), -np.sin(np.pi/6)],     [np.sin(np.pi/6),np.cos(np.pi/6)] ])  gate = qcfpga.Gate(gate_matrix) register = qcfpga.State(3) register.apply_all(gate) np.round(register.probabilities(),3)  Out[5]: <pre>array([0.422, 0.141, 0.141, 0.047, 0.141, 0.047, 0.047, 0.016],\n      dtype=float32)</pre> In\u00a0[6]: Copied! <pre># Create a new quantum register with 2 qubits\nregister = qcfpga.State(2)\n\n# Apply a hadamard (H) gate to the first qubit.\n# You should note that the qubits are zero indexed\nregister.h(0)\n\n# Add a controlled not (CNOT/CX) gate, with the control as\n# the first qubit and target as the second.\n# The register will now be in the bell state.\nregister.cx(0, 1)\n\n# Perform a measurement with 1000 samples\nresults = register.measure(samples=1000)\nresults\n</pre> # Create a new quantum register with 2 qubits register = qcfpga.State(2)  # Apply a hadamard (H) gate to the first qubit. # You should note that the qubits are zero indexed register.h(0)  # Add a controlled not (CNOT/CX) gate, with the control as # the first qubit and target as the second. # The register will now be in the bell state. register.cx(0, 1)  # Perform a measurement with 1000 samples results = register.measure(samples=1000) results Out[6]: <pre>{'11': 502, '00': 498}</pre> In\u00a0[9]: Copied! <pre>import qcfpga\n\nnum_qubits = 20 # The number of qubits to use\na = 70 # The hidden integer, bitstring is 1000110\n\nregister = qcfpga.State(num_qubits) # Create a new quantum register\n\nregister.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit\n\n# Apply the inner products oracle\nfor i in range(num_qubits):\n    if a &amp; (1 &lt;&lt; i) != 0:\n        register.z(i)\nregister.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit\n\nresults = register.measure(samples=1000) # Measure the register (sample 1000 times)\nprint(results)\n</pre> import qcfpga  num_qubits = 20 # The number of qubits to use a = 70 # The hidden integer, bitstring is 1000110  register = qcfpga.State(num_qubits) # Create a new quantum register  register.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit  # Apply the inner products oracle for i in range(num_qubits):     if a &amp; (1 &lt;&lt; i) != 0:         register.z(i) register.apply_all(qcfpga.gate.h()) # Apply a hadamard gate to each qubit  results = register.measure(samples=1000) # Measure the register (sample 1000 times) print(results) <pre>{'00000000000001000110': 1000}\n</pre>"},{"location":"qcfpga/#connecting-to-the-jlab-portal","title":"Connecting to the jlab portal\u00b6","text":"<ol> <li><p>Please connect to the jlab portal first.</p> </li> <li><p>Start a server. Don't forget to select FPGA instead of CPU (see below):</p> </li> </ol> <ol> <li>Select a working directory on the left pane, and then click on Terminal</li> </ol> <ol> <li>Clone the training repository: <code>git clone https://github.com/LuxProvide/QuantumFPGA</code></li> </ol> <ol> <li>Finally, on the left pane, click <code>QuantumFPGA/notebooks/qcfpa.ipynb</code> to open the notebook <code>qcfpa.ipynb</code> in JupyterLab.</li> </ol>"},{"location":"qcfpga/#setting-the-ipython-kernel","title":"Setting the IPython kernel\u00b6","text":"<p>We need now to create a dedicated IPython kernel to be able to run efficiently on efficiently on FPGA node</p> <ul> <li><p>Kernels are by default located in this folder <code>$HOME/.local/share/jupyter/kernels</code></p> </li> <li><p>Execute the following cell to create a custom kernel named QCFPGA</p> </li> </ul>"},{"location":"qcfpga/#the-qcfpga-library","title":"The QCFPGA library\u00b6","text":"<p>QCFPGA is a software library which is a fork of the public QCGPU software that was designed to perform quantum computing simulations on graphics processing units (GPUs) using PyOpenCL. The main idea behind QCFPGA is to utilize the parallel processing capabilities of modern FPGAs to speed up quantum simulations, which are computationally intensive tasks that can benefit greatly from the pipeline parallelism offered by modern FPGAs.</p> <p>The library provides a high-level interface for defining quantum states, applying gates, and performing measurements, much like other quantum computing frameworks. Nonetheless, the library is far from being complete as the Qiskit (IBM) or Cirq (Google).</p> <p>QFPGA was adapted from QCGPU as a proof of concept with the intent to make quantum computing simulations more accessible and faster, leveraging the powerful computational capabilities of FPGAs to handle state vector manipulations typical in quantum computing.</p>  \u26a0\ufe0f  QCFPGA is a Work In Progress and may be subject to changes in the near future. Our main goal is to take advantage of kernel optimization on FPGAs and develop a multi-node version.For any problem, please contact the support team using our  servicedesk portal"},{"location":"qcfpga/#built-in-gates","title":"Built-In Gates\u00b6","text":"<p>In Quantum Computing, gates are used to manipulate quantum registers and to implement quantum algorithms.</p> <p>There are a number of gates built into QCGPU and QCFPGA. They can all be applied the same way:</p>"},{"location":"qcfpga/#applying-a-gate-to-all-qubits-in-parallel","title":"Applying a gate to all qubits in parallel\u00b6","text":""},{"location":"qcfpga/#define-your-own-gate","title":"Define your own gate\u00b6","text":"<ul> <li><p>Custom gates in QCFPGA use the <code>qcfpga.Gate</code> class.</p> </li> <li><p>Only single gate qubits can be defined</p> </li> </ul>  \u26a0\ufe0f  The input to the `Gate` constructor is checked to be a 2x2 unitary matrix."},{"location":"qcfpga/#mesuring-register","title":"Mesuring register\u00b6","text":"<ul> <li>Normally, real qubits will collapse, i.e., become classical qubits after measuring the register</li> <li>For obvious reason, it would require to rebuild a new circuit and repeat the experience</li> </ul>"},{"location":"qcfpga/#use-case-bernstein-vazirani-algorithm","title":"Use case: Bernstein-Vazirani Algorithm\u00b6","text":"<p>The Bernstein-Vazirani algorithm is a quantum algorithm that highlights the superiority of quantum computers in solving specific problems more efficiently than classical computers. This algorithm solves the problem of determining a hidden binary string with minimal queries to a given function.</p>"},{"location":"qcfpga/#problem-setup","title":"Problem Setup\u00b6","text":"<p>You are given a black box function (oracle) that computes:</p> <ul> <li>Function: $ f(x) = a \\cdot x $<ul> <li>a is a hidden string of $ n $ bits.</li> <li>x is an  $n$-bit string.</li> <li>The dot product $a \\cdot x $ is calculated as $ (a_1x_1 + a_2x_2 + \\dots + a_nx_n) $ modulo 2.</li> </ul> </li> <li>Goal: Determine the hidden string $a $ using the fewest number of queries to $f$.</li> </ul>"},{"location":"qcfpga/#classical-approach","title":"Classical Approach\u00b6","text":"<p>Classically, you would need to make $n$ queries to the oracle, each with x set to vectors representing each bit position (e.g., $ 100...0, 010...0, \\ldots, 000...1 $), revealing one bit of $ a $ per query.</p>"},{"location":"qcfpga/#quantum-solution","title":"Quantum Solution\u00b6","text":"<p>The Bernstein-Vazirani algorithm uses a quantum computer to identify $ a $ with a single query, showing an exponential improvement in query complexity.</p>"},{"location":"qcfpga/#steps-of-the-algorithm","title":"Steps of the Algorithm:\u00b6","text":"<ol> <li><p>Initialization: Start with $ n $ qubits in the state $ |0\\rangle $ and one auxiliary qubit in the state $|1\\rangle $.</p> </li> <li><p>Apply Hadamard Gates: Apply Hadamard gates to all qubits, transforming each $ |0\\rangle $to $ \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} $ and $ |1\\rangle $ to $\\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}$.</p> </li> <li><p>Query the Oracle: The function $ f(x) $ modifies the auxiliary qubit by $ (-1)^{f(x)} $, encoding the dot product $ a \\cdot x $ in the quantum state.</p> </li> <li><p>Apply Hadamard Gates Again: Applying Hadamard gates again to all but the auxiliary qubit uses quantum interference to amplify the probability amplitudes of the states corresponding to $ a$.</p> </li> <li><p>Measurement: Measure the first $ n $ qubits to directly obtain $a $ in binary form.</p> </li> </ol>"},{"location":"qcfpga/#conclusion-and-significance","title":"Conclusion and Significance\u00b6","text":"<p>The Bernstein-Vazirani algorithm demonstrates quantum parallelism and serves as an introductory example for more complex quantum algorithms like Shor's and Grover's algorithms, highlighting quantum computational speed-ups.</p>"},{"location":"writing/","title":"Developing SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"writing/#anatomy-of-a-sycl-program","title":"Anatomy of a SYCL program","text":""},{"location":"writing/#data-management","title":"Data Management","text":"<p>In the context of SYCL, Unified Shared Memory (USM) and buffers represent two different ways to handle memory and data management. They offer different levels of abstraction and ease of use, and the choice between them may depend on the specific needs of an application. Here's a breakdown of the differences:</p>"},{"location":"writing/#unified-shared-memory-usm","title":"Unified Shared Memory (USM)","text":"<p>Unified Shared Memory is a feature that simplifies memory management by providing a shared memory space across the host and various devices, like CPUs, GPUs, and FPGAs. USM provides three different types of allocations:</p> <ol> <li>Device Allocations: Allocated memory is accessible only by the device.</li> <li>Host Allocations: Allocated memory is accessible by the host and can be accessed by devices. However, the allocated memory is stored on the host global memory. </li> <li>Shared Allocations: Allocated memory is accessible by both the host and devices. The allocated memory is present in both global memories and it is synchronized between host and device.</li> </ol> <p>USM allows for more straightforward coding, akin to standard C++ memory management, and may lead to code that is easier to write and maintain. </p> <p>FPGA support</p> <p>SYCL USM host allocations are only supported by some BSPs, such as the Intel\u00ae FPGA Programmable Acceleration Card (PAC) D5005 (previously known as Intel\u00ae FPGA Programmable Acceleration Card (PAC) with Intel\u00ae Stratix\u00ae 10 SX FPGA).</p> <p>Using SYCL, you can verify if you have access to the different features:</p> <p>Verify USM capabilities</p> <pre><code>if (!device.has(sycl::aspect::usm_shared_allocations)) {\n# Try to default to host allocation only\nif (!device.has(sycl::aspect::usm_host_allocations)) {\n# Default to device and explicit data movement\nstd::array&lt;int,N&gt; host_array;\nint *my_array = malloc_device&lt;int&gt;(N, Q);\n}else{\n# Ok my_array is located on host memory but transferred to device as needed\nint* my_array = malloc_host&lt;int&gt;(N, Q);\n}\n}else{\n# Ok my_array is located on both global memories and synchronized automatically \nint* shared_array = malloc_shared&lt;int&gt;(N, Q);\n}\n</code></pre> <p>That's not all</p> <ul> <li>Concurrent accesses and atomic modifications are not necessarily available even if you have host and shared capabilities.</li> <li>You need to verify <code>aspect::usm_atomic_shared_allocations</code> and <code>aspect::usm_atomic_host_allocations</code>.</li> </ul> <p>Bittware 520N-MX</p> <p>The USM host allocations is not supported by some BSPs. We will therefore use explicit data movement.</p> <p>Explicit USM</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Replace the original code with explicit USM code </li> <li>Verify your code using emulation</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\nint len) {\nfor (int idx = 0; idx &lt; len; idx++) {\nint a_val = vec_a_in[idx];\nint b_val = vec_b_in[idx];\nint sum = a_val + b_val;\nvec_c_out[idx] = sum;\n}\n}\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint host_vec_a[kVectSize];\nint host_vec_b[kVectSize];\nint host_vec_c[kVectSize];\nint * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\nint * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\nint * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\nfor (int i = 0; i &lt; kVectSize; i++) {\nhost_vec_a[i] = i;\nhost_vec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\nq.memcpy(vec_a, host_vec_a, kVectSize * sizeof(int)).wait();\nq.memcpy(vec_b, host_vec_b, kVectSize * sizeof(int)).wait();\nq.single_task&lt;VectorAddID&gt;([=]() {\nVectorAdd(vec_a, vec_b, vec_c, kVectSize);\n}).wait();\nq.memcpy(host_vec_c, vec_c, kVectSize * sizeof(int)).wait();\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = host_vec_a[i] + host_vec_b[i];\nif (host_vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; host_vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; host_vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; host_vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\nsycl::free(vec_a,q);\nsycl::free(vec_b,q);\nsycl::free(vec_c,q);\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#buffer-accessors","title":"Buffer &amp; accessors","text":"<p>Buffers and accessors are key abstractions that enable memory management and data access across various types of devices like CPUs, GPUs, DSPs, etc.</p> <ol> <li> <p>Buffers:Buffers in SYCL are objects that represent a region of memory accessible by the runtime. They act as containers for data and provide a way to abstract the memory management across host and device memories. This allows for efficient data movement and optimization by the runtime, as it can manage the data movement between host and device memory transparently.</p> </li> <li> <p>Accessors:Accessors provide a way to access the data inside buffers. They define the type of access (read, write, read-write) and can be used within kernels to read from or write to buffers.</p> </li> </ol> <p>Advantage</p> <p>Through the utilization of these accessors, the SYCL runtime examines the interactions with the buffers and constructs a dependency graph that maps the relationship between host and device functions. This enables the runtime to automatically orchestrate the transfer of data and the sequencing of kernel activities.</p> <p>Using Buffers and Accessors</p> <pre><code>    #include &lt;array&gt; // oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\nclass Kernel;\nconstexpr int N = 100;\nstd::array&lt;int,N&gt; in_array;\nstd::array&lt;int,N&gt; out_array;\nfor (int i = 0 ; i &lt;N; i++)\nin_array[i] = i+1;\nqueue device_queue(sycl::ext::intel::fpga_selector_v);\n{ // This one is very important to define the buffer scope\n// buffer&lt;int, 1&gt; in_device_buf(in.data(), in.size());\n// Or more convenient\nbuffer in_device_buf(in_array);\nbuffer out_device_buf(out_array);\ndevice_queue.submit([&amp;](handler &amp;h) {\naccessor in(in_device_buf, h, read_only);\naccessor out(out_device_buf, h, write_only, no_init);\nh.single_task&lt;Kernel&gt;([=]() { });\n};\n} // Accessor going out of the scope\n// Data has been copied back !!!\n</code></pre> <p>What about memory accesses in FPGA ? </p> <ul> <li>For FPGAs, the access pattern, access width, and coalescing of memory accesses can significantly affect performance. You might want to make use of various attributes and pragmas specific to your compiler and FPGA to guide the compiler in optimizing memory accesses.</li> <li>In order to use Direct Memory Acces (DMA), you will need to setup proper data alignment or the offline compiler will output the following warnings: <pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\n</code></pre></li> <li>For example, you may need to replace: <pre><code>    int * vec_a = new int[kVectSize];\nint * vec_b = new int[kVectSize];\nint * vec_c = new int[kVectSize];\n</code></pre> by these ones: <pre><code>   int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_c = new(std::align_val_t{ 64 }) int[kVectSize]; </code></pre></li> </ul>"},{"location":"writing/#queue","title":"Queue","text":"<p>Contrary to OpenCL, queues in SYCL are out-of-order by default. Nonetheless, you can change this behavior you declare it in your code.</p> <p>In-order-queue</p> <p> <pre><code>  ... queue device_queue{sycl::ext::intel::fpga_selector_v,{property::queue::in_order()}};\n// Task A\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskA&gt;([=]() { });\n});\n// Task B\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskB&gt;([=]() { });\n});\n// Task C\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskC&gt;([=]() { });\n}); ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nB[TaskB] --&gt; C[TaskC];</code></pre> </p> <p>This behavior is not very useful nor flexible. Queue objects, by default, are out-of-order queues, except when they're constructed with the in-order queue property. Because of this, they must include mechanisms to arrange tasks that are sent to them. The way queues organize tasks is by allowing the user to notify the runtime about the dependencies that exist between these tasks. These dependencies can be described in two ways: either explicitly or implicitly, through the use of command groups.</p> <p>A command group is a specific object that outlines a task and its dependencies. These groups are generally expressed as C++ lambdas and are handed over as arguments to the submit() method within a queue object. The single parameter within this lambda is a reference to a handler object, utilized inside the command group to define actions, generate accessors, and outline dependencies.</p>"},{"location":"writing/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Like for OpenCL, you can manage dependencies explicitly using events. </p> <p>Using events</p> <p> <pre><code>  ... queue device_queue{sycl::ext::intel::fpga_selector_v};\n// Task A\nauto event_A = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskA&gt;([=]() { });\n});\nevent_A.wait();\n// Task B\nauto event_B = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskB&gt;([=]() { });\n});\n// Task C\nauto event_C = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskC&gt;([=]() { });\n});\n// Task D\ndevice_queue.submit([&amp;](handler &amp;h) {\nh.depends_on({event_B, event_C});\nh.parallel_for(N, [=](id&lt;1&gt; i) { /*...*/ });\n}).wait();\n...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nA[TaskA] --&gt; C[TaskC];\nB[TaskB] --&gt; D[TaskD];\nC[TaskC] --&gt; D[TaskD];</code></pre> </p> <ul> <li>Explicit dependencies using events is relevant when you use USM since buffers make use of accessors to model data dependencies.</li> <li>They are three possibilities to declare a dependcies explicitely:</li> <li>Calling the method <code>wait()</code> on the queue it-self</li> <li>Calling the method <code>wait</code> on the event return by the queue after submitting a command</li> <li>Calling the method <code>depends_on</code> of the handler object</li> </ul>"},{"location":"writing/#implicit-dependencies","title":"Implicit dependencies","text":"<ul> <li>Implicit dependencies occurs when your are using buffer &amp; accessor.</li> <li> <p>Accessors have different access modes:</p> </li> <li> <p>read_only: The content of the buffer can only be accessed for reading. So the content will only be copied once to the device</p> </li> <li>write_only: The content of the buffer can only be accessed for writing. The content of buffer is still copied from host to device before the kernel starts </li> <li>read_write: The content of the buffer can be accessed for reading and writing.</li> </ul> <p>You can add the <code>no_init</code> property to an accessor in <code>write_only</code> mode. This tells the runtime that the original data contains in the buffer can be ignored and don't need to be copied from host to device.</p> <p>Implicit dependencies obey to three main patterns (see DPC++ book):</p> <ul> <li>Read-after-Write  (RAW) : occurs when some data modified by a kernel should be read by another kernel. </li> <li>Write-after-Read  (WAR) : occurs when some data read by a kernel will be modified by another one</li> <li>Write-after-Write (WAW) : occurs when two kernels modified the same data</li> </ul> <p>Implicit dependencies</p> QuestionSolution <ul> <li>By default without access mode, each accessor will be read_write inducing unnecessary copies.</li> <li>Note also the first use of <code>host_accessor</code>. Why did we use it here ?</li> <li>Modifiy the following code to take into account implicit dependencies.  <pre><code>   constexpr int N = 100;\nqueue Q;\nbuffer&lt;int&gt; A{range{N}};\nbuffer&lt;int&gt; B{range{N}};\nbuffer&lt;int&gt; C{range{N}};\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h};\naccessor aB{B, h};\naccessor aC{C, h};\nh.single_task&lt;Kernel1&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naA[i] = 10;\naB[i] = 50;\naC[i] = 0;\n});\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h};\naccessor aB{B, h};\naccessor aC{C, h};\nh.single_task&lt;Kernel2&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naC[i] += aA[i] + aB[i]; });\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aC{C, h};\nh.single_task&lt;Kernel3&gt;([=]() {\nfor(unsigned int i =0; i&lt;N; i++)\naC[i]++; });\n});\nhost_accessor result{C};\n</code></pre></li> </ul> <p><pre><code>   constexpr int N = 100;\nqueue Q;\nbuffer&lt;int&gt; A{range{N}};\nbuffer&lt;int&gt; B{range{N}};\nbuffer&lt;int&gt; C{range{N}};\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h, write_only, no_init};\naccessor aB{B, h, write_only, no_init};\naccessor aC{C, h, write_only, no_init};\nh.single_task&lt;Kernel1&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naA[i] = 10;\naB[i] = 50;\naC[i] = 0;\n});\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h, read_only};\naccessor aB{B, h, read_only};\naccessor aC{C, h, write_only};\nh.single_task&lt;Kernel2&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naC[i] += aA[i] + aB[i]; });\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aC{C, h, write_only};\nh.single_task&lt;Kernel3&gt;([=]() {\nfor(unsigned int i =0; i&lt;N; i++)\naC[i]++; });\n});\nhost_accessor result{C, read_only};\n</code></pre> * The <code>host_accessor</code> obtains access to buffer on the host and will wait for device kernel to execute to generate data.</p>"},{"location":"writing/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<ul> <li>FPGA strongly differs from ISA-based hardware such as CPU and GPU</li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize the everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p></p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Pipelining example provided chap.17 (DPC++ book)</p> Processing a single element (Figure. 17-13)Taking advantage of pipelining (Figure 17-14) <p></p> <ul> <li>The pipeline is mostly empty.</li> <li>Hardware units are not busy and the efficiency is thus low.</li> </ul> <p></p> <ul> <li>More data than stages, the pipeline is full and all hardware units are busy.</li> </ul> <p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"writing/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of thread started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16  <ul> <li>In order to write basic data-parallel kernel, you will need to use the <code>parallel_for</code> method. Below is an example of simple data-parallel kernel. As you can notice it, there is no notion of groups nor sub-groups. </li> </ul> <p>Matrix addition</p> <pre><code>   constexpr int N = 2048;\nconstexpr int M = 1024;\nqueue.submit([&amp;](sycl::handler &amp;h) {\nsycl::accessor acc_a{buffer_a, h, sycl::read_only};\nsycl::accessor acc_b{buffer_b, h, sycl::read_only};\nsycl::accessor acc_c{buffer_c, h, sycl::read_write, sycl::no_init};\nh.parallel_for(range{N, M}, [=](sycl::id&lt;2&gt; idx) {\nacc_c[idx] = acc_a[idx] + acc_b[idx];\n});\n});\n</code></pre> <p>Vector addition</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Adapt the <code>vector_add.cpp</code> single-task kernel to a basis data-parallel kernel</li> <li>Emulate to verify your design</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec_a[i] = i;\nvec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\nsycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\nsycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::write_only, sycl::no_init};\nh.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\naccessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = vec_a[i] + vec_b[i];\nif (vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec_a;\ndelete[] vec_b;\ndelete[] vec_c;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>If you want to have a fine-grained control of your data-parallel kernel, ND-range data-parallel kernels are the equivalent of ND-range kernels in OpenCL. </li> </ul> <p>ND-range kernel in SYCL</p> <ul> <li><code>nd_range(range&lt;dimensions&gt; globalSize, range&lt;dimensions&gt; localSize);</code></li> <li>ND-range kernels are defined with two range objects<ul> <li>global representing the total size of work-items</li> <li>local representing the size of work-groups</li> </ul> </li> </ul> <p>Tiled Matrix Multiplication</p> QuestionSolution <ul> <li>Fill the blank and complete the code  <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// initialize input and output memory on the host\nconstexpr size_t N = 512;\nconstexpr size_t B =  16;\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); std::random_device rd;\nstd::mt19937 mt(rd());\nstd::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n// Generate random values\nstd::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// Generate random values\nstd::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// fill with zero\nstd::fill(mat_c.begin(), mat_c.end(), 0.0); std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n{\n// copy the input arrays to buffers to share with kernel\n// We can access the buffer using mat[i][j]\nsycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n/* DEFINE HERE the global size and local size ranges*/\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nsycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\nsycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\nh.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n[[intel::max_work_group_size(1, B, B)]]    {\n// Indices in the global index space:\nint m = item.get_global_id()[0];\nint n = item.get_global_id()[1];\n// Index in the local index space:\n// Provide local indexes i and j -- fill here\nfloat sum = 0;\nfor (int p = 0; p &lt; N/B; p++) {\n// Load the matrix tile from matrix A, and synchronize\n// to ensure all work-items have a consistent view\n// of the matrix tile in local memory.\ntileA[i][j] = accessor_a[m][p*B+j];\n// Do the same for tileB\n// fill here \nitem.barrier();\n// Perform computation using the local memory tile, and\n// matrix B in global memory.\nfor (int kk = 0; kk &lt; B; kk++) {\nsum += tileA[i][kk] * tileB[kk][j];\n}\n// After computation, synchronize again, to ensure all\n// Fill here \n}\n// Write the final result to global memory.\naccessor_c[m][n] = sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Matrix multiplication is correct\nfor (int i = 0; i &lt; N; i++) {\nfor (int j = 0; j &lt; N; j++){\nfloat true_val=0.0;\nfor (int k = 0 ; k &lt; N; k++){\ntrue_val += mat_a[i*N +k] * mat_b[k*N+j];\n}\nif (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\nstd::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\npassed = false;\n}\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// initialize input and output memory on the host\nconstexpr size_t N = 512;\nconstexpr size_t B =  16;\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); std::random_device rd;\nstd::mt19937 mt(rd());\nstd::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n// Generate random values\nstd::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// Generate random values\nstd::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// fill with zero\nstd::fill(mat_c.begin(), mat_c.end(), 0.0); std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n{\n// copy the input arrays to buffers to share with kernel\n// We can access the buffer using mat[i][j]\nsycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::range global {N,N};\nsycl::range local  {B,B}; q.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nsycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\nsycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\nh.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n[[intel::max_work_group_size(1, B, B)]]    {\n// Indices in the global index space:\nint m = item.get_global_id()[0];\nint n = item.get_global_id()[1];\n// Index in the local index space:\nint i = item.get_local_id()[0];\nint j = item.get_local_id()[1];\nfloat sum = 0;\nfor (int p = 0; p &lt; N/B; p++) {\n// Load the matrix tile from matrix A, and synchronize\n// to ensure all work-items have a consistent view\n// of the matrix tile in local memory.\ntileA[i][j] = accessor_a[m][p*B+j];\ntileB[i][j] = accessor_b[p*B+i][n];\nitem.barrier();\n// Perform computation using the local memory tile, and\n// matrix B in global memory.\nfor (int kk = 0; kk &lt; B; kk++) {\nsum += tileA[i][kk] * tileB[kk][j];\n}\n// After computation, synchronize again, to ensure all\n// reads from the local memory tile are complete.\nitem.barrier();\n}\n// Write the final result to global memory.\naccessor_c[m][n] = sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Matrix multiplication is correct\nfor (int i = 0; i &lt; N; i++) {\nfor (int j = 0; j &lt; N; j++){\nfloat true_val=0.0;\nfor (int k = 0 ; k &lt; N; k++){\ntrue_val += mat_a[i*N +k] * mat_b[k*N+j];\n}\nif (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\nstd::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\npassed = false;\n}\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Warning on work-items group size</p> <ul> <li>If the attribute [[intel::max_work_group_size(Z, Y, X)]] is not specified in your kernel, the workgroup size assumes a default value depending on compilation time and runtime constraints</li> <li>If your kernel contains a barrier, the Intel\u00ae oneAPI DPC++/C++ Compiler sets a default maximum scalarized work-group size of 128 work-items ==&gt; without this attribute, the previous ND-Range kernel would have failed since we have a local work-group size of B x B = 256 work-items </li> </ul>"},{"location":"writing/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the programmer need not do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul> <p>Single-work item creation</p> <ul> <li>Replace the <code>parallel_for</code>method by the <code>single_task</code> method defined in the handler class to create a single-work item kernel</li> <li>The source file <code>vector_add.cpp</code> from <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code> uses loop pipelining.</li> </ul> <pre><code>  #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\nusing namespace sycl;\nint main(){\n// queue creation &amp; data initialization\nq.submit([&amp;](handler &amp;h) {\nh.single_task&lt;class MyKernel&gt;([=]() {\n// Code to be executed as a single task\n});\n});\nq.wait();\n}\n</code></pre>"}]}